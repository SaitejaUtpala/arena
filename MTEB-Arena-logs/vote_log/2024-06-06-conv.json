{"tstamp": 1717701007.6846, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701003.9655, "finish": 1717701007.6846, "state": {"conv_id": "d47e92faaa6f48b4bda809b779ce1e7e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Finance", "output": [["Finance", "Title: The O.C. (season 4)\n\nPassage: The fourth and final season of The O.C., an American teen drama television series, aired in the United States from November 2, 2006 to February 22, 2007 and consisted of sixteen episodes. The O.C's final season aired Thursdays at 9:00\u00a0p.m. ET in the United States on Fox, a terrestrial television network.[1] Fox tested a new timeslot by airing the second episode on Wednesday at 9:00\u00a0p.m. ET against ABC's Lost and CBS's Criminal Minds.[2] With Lost about to enter a three-month hiatus, Fox hoped that changing the timeslot for The O.C. would attract back viewers it had lost since the end of the previous season; however, the move was unsuccessful and the show returned to its Thursday timeslot.[3]"]]}, "ip": null}
{"tstamp": 1717701007.6846, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701003.9655, "finish": 1717701007.6846, "state": {"conv_id": "2801a0907b6f4cc7a39ac4909224536d", "model_name": "intfloat/multilingual-e5-small", "prompt": "Finance", "output": [["Finance", "Title: Pumped Up Kicks\n\nPassage: *sales figures based on certification alone\n^shipments figures based on certification alone\nsales+streaming figures based on certification alone"]]}, "ip": null}
{"tstamp": 1717701014.7765, "task_type": "retrieval", "type": "bothbadvote", "models": ["", ""], "states": [{"conv_id": "d47e92faaa6f48b4bda809b779ce1e7e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Finance", "output": [["Finance", "Title: The O.C. (season 4)\n\nPassage: The fourth and final season of The O.C., an American teen drama television series, aired in the United States from November 2, 2006 to February 22, 2007 and consisted of sixteen episodes. The O.C's final season aired Thursdays at 9:00\u00a0p.m. ET in the United States on Fox, a terrestrial television network.[1] Fox tested a new timeslot by airing the second episode on Wednesday at 9:00\u00a0p.m. ET against ABC's Lost and CBS's Criminal Minds.[2] With Lost about to enter a three-month hiatus, Fox hoped that changing the timeslot for The O.C. would attract back viewers it had lost since the end of the previous season; however, the move was unsuccessful and the show returned to its Thursday timeslot.[3]"]]}, {"conv_id": "2801a0907b6f4cc7a39ac4909224536d", "model_name": "intfloat/multilingual-e5-small", "prompt": "Finance", "output": [["Finance", "Title: Pumped Up Kicks\n\nPassage: *sales figures based on certification alone\n^shipments figures based on certification alone\nsales+streaming figures based on certification alone"]]}], "ip": null}
{"tstamp": 1717701021.2656, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701021.159, "finish": 1717701021.2656, "state": {"conv_id": "6cb891905b4e43af847cc70648c3a583", "model_name": "intfloat/multilingual-e5-small", "prompt": "Where lies China?", "output": [["Where lies China?", "Title: Dragon Boat Festival\n\nPassage: The festival was long marked as a cultural festival in China and is a public holiday in Hong Kong, Taiwan and Macau. The People's Republic of China government established in 1949, however, did not officially recognize Duanwu as a public holiday. Beginning in 2005, the government began to plan for the recognition of three traditional holidays, including Duanwu.[12] Since 2008, Duanwu has been celebrated not only as a festival but also as a public holiday in the People's Republic of China.[13] It is unofficially observed by the Chinese communities of Southeast Asia, including Singapore and Malaysia. Equivalent and related official festivals include the Japanese holiday of Children's Day (Japan)."]]}, "ip": null}
{"tstamp": 1717701021.2656, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701021.159, "finish": 1717701021.2656, "state": {"conv_id": "61847ac3b4bd4c2f9397420ba4a3596e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Where lies China?", "output": [["Where lies China?", "Title: Equilateral triangle\n\nPassage: The area formula \n\n\n\nA\n=\n\n\n\n3\n\n4\n\n\n\na\n\n2\n\n\n\n\n{\\displaystyle A={\\frac {\\sqrt {3}}{4}}a^{2}}\n\n in terms of side length a can be derived directly using the Pythagorean theorem or using trigonometry."]]}, "ip": null}
{"tstamp": 1717701023.4984, "task_type": "retrieval", "type": "leftvote", "models": ["", ""], "states": [{"conv_id": "6cb891905b4e43af847cc70648c3a583", "model_name": "intfloat/multilingual-e5-small", "prompt": "Where lies China?", "output": [["Where lies China?", "Title: Dragon Boat Festival\n\nPassage: The festival was long marked as a cultural festival in China and is a public holiday in Hong Kong, Taiwan and Macau. The People's Republic of China government established in 1949, however, did not officially recognize Duanwu as a public holiday. Beginning in 2005, the government began to plan for the recognition of three traditional holidays, including Duanwu.[12] Since 2008, Duanwu has been celebrated not only as a festival but also as a public holiday in the People's Republic of China.[13] It is unofficially observed by the Chinese communities of Southeast Asia, including Singapore and Malaysia. Equivalent and related official festivals include the Japanese holiday of Children's Day (Japan)."]]}, {"conv_id": "61847ac3b4bd4c2f9397420ba4a3596e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Where lies China?", "output": [["Where lies China?", "Title: Equilateral triangle\n\nPassage: The area formula \n\n\n\nA\n=\n\n\n\n3\n\n4\n\n\n\na\n\n2\n\n\n\n\n{\\displaystyle A={\\frac {\\sqrt {3}}{4}}a^{2}}\n\n in terms of side length a can be derived directly using the Pythagorean theorem or using trigonometry."]]}], "ip": null}
{"tstamp": 1717701031.9917, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701031.8715, "finish": 1717701031.9917, "state": {"conv_id": "8f7e1b48cd394dc69f9ac156b28488ed", "model_name": "intfloat/multilingual-e5-small", "prompt": "What is Jupiter?", "output": [["What is Jupiter?", "Title: Sperm\n\nPassage: The mammalian sperm cell consists of a head, neck, a midpiece and a tail. The head contains the nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome, which contains enzymes used for penetrating the female egg. The neck contains the sperm centriole. The midpiece has a central filamentous core with many mitochondria spiralled around it, used for ATP production for the journey through the female cervix, uterus and uterine tubes. The tail or \"flagellum\" executes the lashing movements that propel the spermatocyte.[1]"]]}, "ip": null}
{"tstamp": 1717701031.9917, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701031.8715, "finish": 1717701031.9917, "state": {"conv_id": "2c8ea19fcf6443d7ad61a0a3aa6b1c3d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "What is Jupiter?", "output": [["What is Jupiter?", "Title: Two-party system\n\nPassage: Vigorous struggle between the two factions characterised the period from the Glorious Revolution to the 1715 Hanoverian succession, over the legacy of the overthrow of the Stuart dynasty and the nature of the new constitutional state. This proto two-party system fell into relative abeyance after the accession to the throne of George I and the consequent period of Whig supremacy under Robert Walpole, during which the Tories were systematically purged from high positions in government. However, although the Tories were dismissed from office for half a century, they still retained a measure of party cohesion under William Wyndham and acted as a united, though unavailing, opposition to Whig corruption and scandals. At times they cooperated with the \"Opposition Whigs\", Whigs who were in opposition to the Whig government; however, the ideological gap between the Tories and the Opposition Whigs prevented them from coalescing as a single party."]]}, "ip": null}
{"tstamp": 1717701035.2819, "task_type": "retrieval", "type": "bothbadvote", "models": ["", ""], "states": [{"conv_id": "8f7e1b48cd394dc69f9ac156b28488ed", "model_name": "intfloat/multilingual-e5-small", "prompt": "What is Jupiter?", "output": [["What is Jupiter?", "Title: Sperm\n\nPassage: The mammalian sperm cell consists of a head, neck, a midpiece and a tail. The head contains the nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome, which contains enzymes used for penetrating the female egg. The neck contains the sperm centriole. The midpiece has a central filamentous core with many mitochondria spiralled around it, used for ATP production for the journey through the female cervix, uterus and uterine tubes. The tail or \"flagellum\" executes the lashing movements that propel the spermatocyte.[1]"]]}, {"conv_id": "2c8ea19fcf6443d7ad61a0a3aa6b1c3d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "What is Jupiter?", "output": [["What is Jupiter?", "Title: Two-party system\n\nPassage: Vigorous struggle between the two factions characterised the period from the Glorious Revolution to the 1715 Hanoverian succession, over the legacy of the overthrow of the Stuart dynasty and the nature of the new constitutional state. This proto two-party system fell into relative abeyance after the accession to the throne of George I and the consequent period of Whig supremacy under Robert Walpole, during which the Tories were systematically purged from high positions in government. However, although the Tories were dismissed from office for half a century, they still retained a measure of party cohesion under William Wyndham and acted as a united, though unavailing, opposition to Whig corruption and scandals. At times they cooperated with the \"Opposition Whigs\", Whigs who were in opposition to the Whig government; however, the ideological gap between the Tories and the Opposition Whigs prevented them from coalescing as a single party."]]}], "ip": null}
{"tstamp": 1717701038.3046, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701038.1943, "finish": 1717701038.3046, "state": {"conv_id": "b51d4fb314e142639f68c5c25ff0f1e0", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Sperm", "output": [["Sperm", "Title: The O.C. (season 4)\n\nPassage: In Canada, the fourth season was simulcast on the terrestrial CTV Television Network. Additionally, as a result of an agreement between Warner Bros. and CTV, the first episode was available to be streamed from October 30, 2006 at 12:00\u00a0p.m. ET onwards, through the CTV Broadband Network.[10] In the United Kingdom the season premiered on January 9, 2007 on E4,[11] and in Australia it was broadcast by Network Ten on November 7, 2006 at 8:30\u00a0p.m. (local time).[12]"]]}, "ip": null}
{"tstamp": 1717701038.3046, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701038.1943, "finish": 1717701038.3046, "state": {"conv_id": "ed383912985b4eb2b299fd2ea1cf5339", "model_name": "intfloat/multilingual-e5-small", "prompt": "Sperm", "output": [["Sperm", "Title: Sperm\n\nPassage: Sperm is the male reproductive cell and is derived from the Greek word (\u03c3\u03c0\u03ad\u03c1\u03bc\u03b1) sperma (meaning \"seed\"). In the types of sexual reproduction known as anisogamy and its subtype oogamy, there is a marked difference in the size of the gametes with the smaller one being termed the \"male\" or sperm cell. A uniflagellar sperm cell that is motile is referred to as a spermatozoon, whereas a non-motile sperm cell is referred to as a spermatium. Sperm cells cannot divide and have a limited life span, but after fusion with egg cells during fertilization, a new organism begins developing, starting as a totipotent zygote. The human sperm cell is haploid, so that its 23 chromosomes can join the 23 chromosomes of the female egg to form a diploid cell. In mammals, sperm develops in the testicles, stored in the epididymis, and is released from the penis."]]}, "ip": null}
{"tstamp": 1717701041.25, "task_type": "retrieval", "type": "rightvote", "models": ["", ""], "states": [{"conv_id": "b51d4fb314e142639f68c5c25ff0f1e0", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Sperm", "output": [["Sperm", "Title: The O.C. (season 4)\n\nPassage: In Canada, the fourth season was simulcast on the terrestrial CTV Television Network. Additionally, as a result of an agreement between Warner Bros. and CTV, the first episode was available to be streamed from October 30, 2006 at 12:00\u00a0p.m. ET onwards, through the CTV Broadband Network.[10] In the United Kingdom the season premiered on January 9, 2007 on E4,[11] and in Australia it was broadcast by Network Ten on November 7, 2006 at 8:30\u00a0p.m. (local time).[12]"]]}, {"conv_id": "ed383912985b4eb2b299fd2ea1cf5339", "model_name": "intfloat/multilingual-e5-small", "prompt": "Sperm", "output": [["Sperm", "Title: Sperm\n\nPassage: Sperm is the male reproductive cell and is derived from the Greek word (\u03c3\u03c0\u03ad\u03c1\u03bc\u03b1) sperma (meaning \"seed\"). In the types of sexual reproduction known as anisogamy and its subtype oogamy, there is a marked difference in the size of the gametes with the smaller one being termed the \"male\" or sperm cell. A uniflagellar sperm cell that is motile is referred to as a spermatozoon, whereas a non-motile sperm cell is referred to as a spermatium. Sperm cells cannot divide and have a limited life span, but after fusion with egg cells during fertilization, a new organism begins developing, starting as a totipotent zygote. The human sperm cell is haploid, so that its 23 chromosomes can join the 23 chromosomes of the female egg to form a diploid cell. In mammals, sperm develops in the testicles, stored in the epididymis, and is released from the penis."]]}], "ip": null}
{"tstamp": 1717701045.5062, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701045.429, "finish": 1717701045.5062, "state": {"conv_id": "442b66b4b66f4dc7a78d7600f84c3e20", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Canada", "output": [["Canada", "Title: Comanche\n\nPassage: The tribe operates its own housing authority and issues tribal vehicle tags. They have their own Department of Higher Education, primarily awarding scholarships and financial aid for members' college educations. Additionally, they operate the Comanche Nation College in Lawton, Oklahoma. They own ten tribal smoke shops and four casinos.[1] The casinos are Comanche Nation Casino in Lawton; Comanche Red River Casino in Devol; Comanche Spur Casino, in Elgin; and Comanche Star Casino in Walters, Oklahoma.[9]"]]}, "ip": null}
{"tstamp": 1717701045.5062, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701045.429, "finish": 1717701045.5062, "state": {"conv_id": "263fbb4bcd7e4fda82bb593964e61421", "model_name": "intfloat/multilingual-e5-small", "prompt": "Canada", "output": [["Canada", "Title: Tobacco advertising\n\nPassage: Tobacco advertising is the advertising of tobacco products or use (typically cigarette smoking) by the tobacco industry through a variety of media including sponsorship, particularly of sporting events. It is now one of the most highly regulated forms of marketing. Some or all forms of tobacco advertising are banned in many countries."]]}, "ip": null}
{"tstamp": 1717701054.2633, "task_type": "retrieval", "type": "bothbadvote", "models": ["", ""], "states": [{"conv_id": "442b66b4b66f4dc7a78d7600f84c3e20", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Canada", "output": [["Canada", "Title: Comanche\n\nPassage: The tribe operates its own housing authority and issues tribal vehicle tags. They have their own Department of Higher Education, primarily awarding scholarships and financial aid for members' college educations. Additionally, they operate the Comanche Nation College in Lawton, Oklahoma. They own ten tribal smoke shops and four casinos.[1] The casinos are Comanche Nation Casino in Lawton; Comanche Red River Casino in Devol; Comanche Spur Casino, in Elgin; and Comanche Star Casino in Walters, Oklahoma.[9]"]]}, {"conv_id": "263fbb4bcd7e4fda82bb593964e61421", "model_name": "intfloat/multilingual-e5-small", "prompt": "Canada", "output": [["Canada", "Title: Tobacco advertising\n\nPassage: Tobacco advertising is the advertising of tobacco products or use (typically cigarette smoking) by the tobacco industry through a variety of media including sponsorship, particularly of sporting events. It is now one of the most highly regulated forms of marketing. Some or all forms of tobacco advertising are banned in many countries."]]}], "ip": null}
{"tstamp": 1717701060.2901, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701060.1818, "finish": 1717701060.2901, "state": {"conv_id": "889560a042c64876a6dae84bea978948", "model_name": "intfloat/multilingual-e5-small", "prompt": "Mathematics", "output": [["Mathematics", "Title: Equilateral triangle\n\nPassage: which is the optic equation."]]}, "ip": null}
{"tstamp": 1717701060.2901, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701060.1818, "finish": 1717701060.2901, "state": {"conv_id": "5bbeca95252241d4b1b25df84c9a5c3b", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Mathematics", "output": [["Mathematics", "Title: Love Will Keep Us Alive\n\nPassage: Aside from being on the album Hell Freezes Over, the song appears on the Eagles' box set, Selected Works 1972-1999 and the 2003 compilation album, The Very Best Of."]]}, "ip": null}
{"tstamp": 1717701062.8379, "task_type": "retrieval", "type": "leftvote", "models": ["", ""], "states": [{"conv_id": "889560a042c64876a6dae84bea978948", "model_name": "intfloat/multilingual-e5-small", "prompt": "Mathematics", "output": [["Mathematics", "Title: Equilateral triangle\n\nPassage: which is the optic equation."]]}, {"conv_id": "5bbeca95252241d4b1b25df84c9a5c3b", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Mathematics", "output": [["Mathematics", "Title: Love Will Keep Us Alive\n\nPassage: Aside from being on the album Hell Freezes Over, the song appears on the Eagles' box set, Selected Works 1972-1999 and the 2003 compilation album, The Very Best Of."]]}], "ip": null}
{"tstamp": 1717701065.8069, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701065.742, "finish": 1717701065.8069, "state": {"conv_id": "920688b1567148dba46af038e9776be7", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Love", "output": [["Love", "Title: Comanche\n\nPassage: The tribe operates its own housing authority and issues tribal vehicle tags. They have their own Department of Higher Education, primarily awarding scholarships and financial aid for members' college educations. Additionally, they operate the Comanche Nation College in Lawton, Oklahoma. They own ten tribal smoke shops and four casinos.[1] The casinos are Comanche Nation Casino in Lawton; Comanche Red River Casino in Devol; Comanche Spur Casino, in Elgin; and Comanche Star Casino in Walters, Oklahoma.[9]"]]}, "ip": null}
{"tstamp": 1717701065.8069, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701065.742, "finish": 1717701065.8069, "state": {"conv_id": "e592617d2ed9468a94ba5b42e2c21fa0", "model_name": "intfloat/multilingual-e5-small", "prompt": "Love", "output": [["Love", "Title: Can't Help Falling in Love\n\nPassage: 7\" single"]]}, "ip": null}
{"tstamp": 1717701068.7604, "task_type": "retrieval", "type": "rightvote", "models": ["", ""], "states": [{"conv_id": "920688b1567148dba46af038e9776be7", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Love", "output": [["Love", "Title: Comanche\n\nPassage: The tribe operates its own housing authority and issues tribal vehicle tags. They have their own Department of Higher Education, primarily awarding scholarships and financial aid for members' college educations. Additionally, they operate the Comanche Nation College in Lawton, Oklahoma. They own ten tribal smoke shops and four casinos.[1] The casinos are Comanche Nation Casino in Lawton; Comanche Red River Casino in Devol; Comanche Spur Casino, in Elgin; and Comanche Star Casino in Walters, Oklahoma.[9]"]]}, {"conv_id": "e592617d2ed9468a94ba5b42e2c21fa0", "model_name": "intfloat/multilingual-e5-small", "prompt": "Love", "output": [["Love", "Title: Can't Help Falling in Love\n\nPassage: 7\" single"]]}], "ip": null}
{"tstamp": 1717701075.7018, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701075.2328, "finish": 1717701075.7018, "state": {"conv_id": "303f1723053b455a94da0c5770ca7254", "model_name": "intfloat/multilingual-e5-small", "prompt": ["love"], "output": null}, "ip": null}
{"tstamp": 1717701075.7018, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701075.2328, "finish": 1717701075.7018, "state": {"conv_id": "025486c5374747cb9336645ba4eb11bb", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["love"], "output": null}, "ip": null}
{"tstamp": 1717701086.5871, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701086.1271, "finish": 1717701086.5871, "state": {"conv_id": "303f1723053b455a94da0c5770ca7254", "model_name": "intfloat/multilingual-e5-small", "prompt": ["love", "\u7231"], "output": null}, "ip": null}
{"tstamp": 1717701086.5871, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701086.1271, "finish": 1717701086.5871, "state": {"conv_id": "025486c5374747cb9336645ba4eb11bb", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["love", "\u7231"], "output": null}, "ip": null}
{"tstamp": 1717701092.2797, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701092.0829, "finish": 1717701092.2797, "state": {"conv_id": "303f1723053b455a94da0c5770ca7254", "model_name": "intfloat/multilingual-e5-small", "prompt": ["love", "\u7231", "hate"], "output": null}, "ip": null}
{"tstamp": 1717701092.2797, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701092.0829, "finish": 1717701092.2797, "state": {"conv_id": "025486c5374747cb9336645ba4eb11bb", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["love", "\u7231", "hate"], "output": null}, "ip": null}
{"tstamp": 1717701114.9618, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701114.7587, "finish": 1717701114.9618, "state": {"conv_id": "303f1723053b455a94da0c5770ca7254", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["love", "\u7231", "hate", "\u604b"], "output": null}, "ip": null}
{"tstamp": 1717701114.9618, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701114.7587, "finish": 1717701114.9618, "state": {"conv_id": "025486c5374747cb9336645ba4eb11bb", "model_name": "intfloat/multilingual-e5-small", "prompt": ["love", "\u7231", "hate", "\u604b"], "output": null}, "ip": null}
{"tstamp": 1717701134.5559, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701134.2941, "finish": 1717701134.5559, "state": {"conv_id": "303f1723053b455a94da0c5770ca7254", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["love", "\u7231", "hate", "\u604b", "\u611b"], "output": null}, "ip": null}
{"tstamp": 1717701134.5559, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701134.2941, "finish": 1717701134.5559, "state": {"conv_id": "025486c5374747cb9336645ba4eb11bb", "model_name": "intfloat/multilingual-e5-small", "prompt": ["love", "\u7231", "hate", "\u604b", "\u611b"], "output": null}, "ip": null}
{"tstamp": 1717701143.8442, "task_type": "clustering", "type": "rightvote", "models": ["", ""], "states": [{"conv_id": "303f1723053b455a94da0c5770ca7254", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["love", "\u7231", "hate", "\u604b", "\u611b"], "output": null}, {"conv_id": "025486c5374747cb9336645ba4eb11bb", "model_name": "intfloat/multilingual-e5-small", "prompt": ["love", "\u7231", "hate", "\u604b", "\u611b"], "output": null}], "ip": null}
{"tstamp": 1717701150.2578, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701150.0264, "finish": 1717701150.2578, "state": {"conv_id": "a4de26ccb96a4724be0c68945acba665", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Hello"], "output": null}, "ip": null}
{"tstamp": 1717701150.2578, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701150.0264, "finish": 1717701150.2578, "state": {"conv_id": "a296e4202dce4ad28757c2672ef9b1b8", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Hello"], "output": null}, "ip": null}
{"tstamp": 1717701154.2411, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701154.0573, "finish": 1717701154.2411, "state": {"conv_id": "a4de26ccb96a4724be0c68945acba665", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Hello", "Good morning"], "output": null}, "ip": null}
{"tstamp": 1717701154.2411, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701154.0573, "finish": 1717701154.2411, "state": {"conv_id": "a296e4202dce4ad28757c2672ef9b1b8", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Hello", "Good morning"], "output": null}, "ip": null}
{"tstamp": 1717701159.8358, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701159.6909, "finish": 1717701159.8358, "state": {"conv_id": "a4de26ccb96a4724be0c68945acba665", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Hello", "Good morning", "morning Good"], "output": null}, "ip": null}
{"tstamp": 1717701159.8358, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701159.6909, "finish": 1717701159.8358, "state": {"conv_id": "a296e4202dce4ad28757c2672ef9b1b8", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Hello", "Good morning", "morning Good"], "output": null}, "ip": null}
{"tstamp": 1717701162.1422, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701161.9911, "finish": 1717701162.1422, "state": {"conv_id": "a4de26ccb96a4724be0c68945acba665", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Hello", "Good morning", "morning Good", "Hi"], "output": null}, "ip": null}
{"tstamp": 1717701162.1422, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701161.9911, "finish": 1717701162.1422, "state": {"conv_id": "a296e4202dce4ad28757c2672ef9b1b8", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Hello", "Good morning", "morning Good", "Hi"], "output": null}, "ip": null}
{"tstamp": 1717701166.106, "task_type": "clustering", "type": "tievote", "models": ["", ""], "states": [{"conv_id": "a4de26ccb96a4724be0c68945acba665", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Hello", "Good morning", "morning Good", "Hi"], "output": null}, {"conv_id": "a296e4202dce4ad28757c2672ef9b1b8", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Hello", "Good morning", "morning Good", "Hi"], "output": null}], "ip": null}
{"tstamp": 1717701169.4345, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701169.321, "finish": 1717701169.4345, "state": {"conv_id": "d605758e45e440059544f2fdc3d25bbf", "model_name": "intfloat/multilingual-e5-small", "prompt": ["where"], "output": null}, "ip": null}
{"tstamp": 1717701169.4345, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701169.321, "finish": 1717701169.4345, "state": {"conv_id": "486f6b5df61d4bcaa629dab9fac3120f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["where"], "output": null}, "ip": null}
{"tstamp": 1717701171.6133, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701171.4762, "finish": 1717701171.6133, "state": {"conv_id": "d605758e45e440059544f2fdc3d25bbf", "model_name": "intfloat/multilingual-e5-small", "prompt": ["where", "what"], "output": null}, "ip": null}
{"tstamp": 1717701171.6133, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701171.4762, "finish": 1717701171.6133, "state": {"conv_id": "486f6b5df61d4bcaa629dab9fac3120f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["where", "what"], "output": null}, "ip": null}
{"tstamp": 1717701173.8572, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701173.7265, "finish": 1717701173.8572, "state": {"conv_id": "d605758e45e440059544f2fdc3d25bbf", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["where", "what", "when"], "output": null}, "ip": null}
{"tstamp": 1717701173.8572, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701173.7265, "finish": 1717701173.8572, "state": {"conv_id": "486f6b5df61d4bcaa629dab9fac3120f", "model_name": "intfloat/multilingual-e5-small", "prompt": ["where", "what", "when"], "output": null}, "ip": null}
{"tstamp": 1717701175.2317, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701175.0997, "finish": 1717701175.2317, "state": {"conv_id": "d605758e45e440059544f2fdc3d25bbf", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["where", "what", "when", "who"], "output": null}, "ip": null}
{"tstamp": 1717701175.2317, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701175.0997, "finish": 1717701175.2317, "state": {"conv_id": "486f6b5df61d4bcaa629dab9fac3120f", "model_name": "intfloat/multilingual-e5-small", "prompt": ["where", "what", "when", "who"], "output": null}, "ip": null}
{"tstamp": 1717701179.3715, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701179.2311, "finish": 1717701179.3715, "state": {"conv_id": "d605758e45e440059544f2fdc3d25bbf", "model_name": "intfloat/multilingual-e5-small", "prompt": ["where", "what", "when", "who", "w"], "output": null}, "ip": null}
{"tstamp": 1717701179.3715, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701179.2311, "finish": 1717701179.3715, "state": {"conv_id": "486f6b5df61d4bcaa629dab9fac3120f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["where", "what", "when", "who", "w"], "output": null}, "ip": null}
{"tstamp": 1717701199.3107, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701199.1311, "finish": 1717701199.3107, "state": {"conv_id": "d605758e45e440059544f2fdc3d25bbf", "model_name": "intfloat/multilingual-e5-small", "prompt": ["where", "what", "when", "who", "w", "ww"], "output": null}, "ip": null}
{"tstamp": 1717701199.3107, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701199.1311, "finish": 1717701199.3107, "state": {"conv_id": "486f6b5df61d4bcaa629dab9fac3120f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["where", "what", "when", "who", "w", "ww"], "output": null}, "ip": null}
{"tstamp": 1717701207.6374, "task_type": "clustering", "type": "leftvote", "models": ["", ""], "states": [{"conv_id": "d605758e45e440059544f2fdc3d25bbf", "model_name": "intfloat/multilingual-e5-small", "prompt": ["where", "what", "when", "who", "w", "ww"], "output": null}, {"conv_id": "486f6b5df61d4bcaa629dab9fac3120f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["where", "what", "when", "who", "w", "ww"], "output": null}], "ip": null}
{"tstamp": 1717701211.3868, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701211.2646, "finish": 1717701211.3868, "state": {"conv_id": "97cc863cbdfa4510b8b387e92bb85070", "model_name": "intfloat/multilingual-e5-small", "prompt": ["line"], "output": null}, "ip": null}
{"tstamp": 1717701211.3868, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701211.2646, "finish": 1717701211.3868, "state": {"conv_id": "2725e4c4281a475583fe5c0b353b8d2f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["line"], "output": null}, "ip": null}
{"tstamp": 1717701220.2345, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701220.1062, "finish": 1717701220.2345, "state": {"conv_id": "97cc863cbdfa4510b8b387e92bb85070", "model_name": "intfloat/multilingual-e5-small", "prompt": ["line", "curve"], "output": null}, "ip": null}
{"tstamp": 1717701220.2345, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701220.1062, "finish": 1717701220.2345, "state": {"conv_id": "2725e4c4281a475583fe5c0b353b8d2f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["line", "curve"], "output": null}, "ip": null}
{"tstamp": 1717701224.9281, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701224.808, "finish": 1717701224.9281, "state": {"conv_id": "97cc863cbdfa4510b8b387e92bb85070", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["line", "curve", "triangle"], "output": null}, "ip": null}
{"tstamp": 1717701224.9281, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701224.808, "finish": 1717701224.9281, "state": {"conv_id": "2725e4c4281a475583fe5c0b353b8d2f", "model_name": "intfloat/multilingual-e5-small", "prompt": ["line", "curve", "triangle"], "output": null}, "ip": null}
{"tstamp": 1717701226.8016, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701226.6889, "finish": 1717701226.8016, "state": {"conv_id": "97cc863cbdfa4510b8b387e92bb85070", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["line", "curve", "triangle", "square"], "output": null}, "ip": null}
{"tstamp": 1717701226.8016, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701226.6889, "finish": 1717701226.8016, "state": {"conv_id": "2725e4c4281a475583fe5c0b353b8d2f", "model_name": "intfloat/multilingual-e5-small", "prompt": ["line", "curve", "triangle", "square"], "output": null}, "ip": null}
{"tstamp": 1717701235.9838, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701235.7828, "finish": 1717701235.9838, "state": {"conv_id": "97cc863cbdfa4510b8b387e92bb85070", "model_name": "intfloat/multilingual-e5-small", "prompt": ["line", "curve", "triangle", "square", "japan"], "output": null}, "ip": null}
{"tstamp": 1717701235.9838, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701235.7828, "finish": 1717701235.9838, "state": {"conv_id": "2725e4c4281a475583fe5c0b353b8d2f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["line", "curve", "triangle", "square", "japan"], "output": null}, "ip": null}
{"tstamp": 1717701237.4687, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701237.3148, "finish": 1717701237.4687, "state": {"conv_id": "97cc863cbdfa4510b8b387e92bb85070", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["line", "curve", "triangle", "square", "japan", "china"], "output": null}, "ip": null}
{"tstamp": 1717701237.4687, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701237.3148, "finish": 1717701237.4687, "state": {"conv_id": "2725e4c4281a475583fe5c0b353b8d2f", "model_name": "intfloat/multilingual-e5-small", "prompt": ["line", "curve", "triangle", "square", "japan", "china"], "output": null}, "ip": null}
{"tstamp": 1717701241.8441, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701241.6523, "finish": 1717701241.8441, "state": {"conv_id": "97cc863cbdfa4510b8b387e92bb85070", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["line", "curve", "triangle", "square", "japan", "china", "thailand"], "output": null}, "ip": null}
{"tstamp": 1717701241.8441, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701241.6523, "finish": 1717701241.8441, "state": {"conv_id": "2725e4c4281a475583fe5c0b353b8d2f", "model_name": "intfloat/multilingual-e5-small", "prompt": ["line", "curve", "triangle", "square", "japan", "china", "thailand"], "output": null}, "ip": null}
{"tstamp": 1717701257.7889, "task_type": "clustering", "type": "leftvote", "models": ["", ""], "states": [{"conv_id": "97cc863cbdfa4510b8b387e92bb85070", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["line", "curve", "triangle", "square", "japan", "china", "thailand"], "output": null}, {"conv_id": "2725e4c4281a475583fe5c0b353b8d2f", "model_name": "intfloat/multilingual-e5-small", "prompt": ["line", "curve", "triangle", "square", "japan", "china", "thailand"], "output": null}], "ip": null}
{"tstamp": 1717701267.4252, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701267.175, "finish": 1717701267.4252, "state": {"conv_id": "70479b212e4346a385af536a8f8586a1", "model_name": "intfloat/multilingual-e5-small", "prompt": ["car"], "output": null}, "ip": null}
{"tstamp": 1717701267.4252, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701267.175, "finish": 1717701267.4252, "state": {"conv_id": "e93c7da10d4040e6b88e964447d0c22c", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["car"], "output": null}, "ip": null}
{"tstamp": 1717701270.2163, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701270.0857, "finish": 1717701270.2163, "state": {"conv_id": "70479b212e4346a385af536a8f8586a1", "model_name": "intfloat/multilingual-e5-small", "prompt": ["car", "bicycle"], "output": null}, "ip": null}
{"tstamp": 1717701270.2163, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701270.0857, "finish": 1717701270.2163, "state": {"conv_id": "e93c7da10d4040e6b88e964447d0c22c", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["car", "bicycle"], "output": null}, "ip": null}
{"tstamp": 1717701276.9048, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701276.7278, "finish": 1717701276.9048, "state": {"conv_id": "70479b212e4346a385af536a8f8586a1", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["car", "bicycle", "scooter"], "output": null}, "ip": null}
{"tstamp": 1717701276.9048, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701276.7278, "finish": 1717701276.9048, "state": {"conv_id": "e93c7da10d4040e6b88e964447d0c22c", "model_name": "intfloat/multilingual-e5-small", "prompt": ["car", "bicycle", "scooter"], "output": null}, "ip": null}
{"tstamp": 1717701282.8737, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701282.7077, "finish": 1717701282.8737, "state": {"conv_id": "70479b212e4346a385af536a8f8586a1", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["car", "bicycle", "scooter", "doctor"], "output": null}, "ip": null}
{"tstamp": 1717701282.8737, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701282.7077, "finish": 1717701282.8737, "state": {"conv_id": "e93c7da10d4040e6b88e964447d0c22c", "model_name": "intfloat/multilingual-e5-small", "prompt": ["car", "bicycle", "scooter", "doctor"], "output": null}, "ip": null}
{"tstamp": 1717701284.8305, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701284.6628, "finish": 1717701284.8305, "state": {"conv_id": "70479b212e4346a385af536a8f8586a1", "model_name": "intfloat/multilingual-e5-small", "prompt": ["car", "bicycle", "scooter", "doctor", "nurse"], "output": null}, "ip": null}
{"tstamp": 1717701284.8305, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701284.6628, "finish": 1717701284.8305, "state": {"conv_id": "e93c7da10d4040e6b88e964447d0c22c", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["car", "bicycle", "scooter", "doctor", "nurse"], "output": null}, "ip": null}
{"tstamp": 1717701289.0085, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701288.8665, "finish": 1717701289.0085, "state": {"conv_id": "70479b212e4346a385af536a8f8586a1", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["car", "bicycle", "scooter", "doctor", "nurse", "lawyer"], "output": null}, "ip": null}
{"tstamp": 1717701289.0085, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701288.8665, "finish": 1717701289.0085, "state": {"conv_id": "e93c7da10d4040e6b88e964447d0c22c", "model_name": "intfloat/multilingual-e5-small", "prompt": ["car", "bicycle", "scooter", "doctor", "nurse", "lawyer"], "output": null}, "ip": null}
{"tstamp": 1717701306.7372, "task_type": "clustering", "type": "leftvote", "models": ["", ""], "states": [{"conv_id": "70479b212e4346a385af536a8f8586a1", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["car", "bicycle", "scooter", "doctor", "nurse", "lawyer"], "output": null}, {"conv_id": "e93c7da10d4040e6b88e964447d0c22c", "model_name": "intfloat/multilingual-e5-small", "prompt": ["car", "bicycle", "scooter", "doctor", "nurse", "lawyer"], "output": null}], "ip": null}
{"tstamp": 1717701317.6298, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701317.4588, "finish": 1717701317.6298, "state": {"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple"], "output": null}, "ip": null}
{"tstamp": 1717701317.6298, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701317.4588, "finish": 1717701317.6298, "state": {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple"], "output": null}, "ip": null}
{"tstamp": 1717701320.9001, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701320.6788, "finish": 1717701320.9001, "state": {"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple", "apple"], "output": null}, "ip": null}
{"tstamp": 1717701320.9001, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701320.6788, "finish": 1717701320.9001, "state": {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple", "apple"], "output": null}, "ip": null}
{"tstamp": 1717701322.9212, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701322.77, "finish": 1717701322.9212, "state": {"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple", "apple", "mango"], "output": null}, "ip": null}
{"tstamp": 1717701322.9212, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701322.77, "finish": 1717701322.9212, "state": {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple", "apple", "mango"], "output": null}, "ip": null}
{"tstamp": 1717701324.8433, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701324.7055, "finish": 1717701324.8433, "state": {"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple", "apple", "mango", "peach"], "output": null}, "ip": null}
{"tstamp": 1717701324.8433, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701324.7055, "finish": 1717701324.8433, "state": {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple", "apple", "mango", "peach"], "output": null}, "ip": null}
{"tstamp": 1717701329.3262, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701329.1562, "finish": 1717701329.3262, "state": {"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple", "apple", "mango", "peach", "lemon"], "output": null}, "ip": null}
{"tstamp": 1717701329.3262, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701329.1562, "finish": 1717701329.3262, "state": {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple", "apple", "mango", "peach", "lemon"], "output": null}, "ip": null}
{"tstamp": 1717701334.1555, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701334.0059, "finish": 1717701334.1555, "state": {"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot"], "output": null}, "ip": null}
{"tstamp": 1717701334.1555, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701334.0059, "finish": 1717701334.1555, "state": {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot"], "output": null}, "ip": null}
{"tstamp": 1717701336.1727, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701335.9921, "finish": 1717701336.1727, "state": {"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot", "tomato"], "output": null}, "ip": null}
{"tstamp": 1717701336.1727, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701335.9921, "finish": 1717701336.1727, "state": {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot", "tomato"], "output": null}, "ip": null}
{"tstamp": 1717701338.4113, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701338.2451, "finish": 1717701338.4113, "state": {"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot", "tomato", "broccoli"], "output": null}, "ip": null}
{"tstamp": 1717701338.4113, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701338.2451, "finish": 1717701338.4113, "state": {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot", "tomato", "broccoli"], "output": null}, "ip": null}
{"tstamp": 1717701353.4065, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701353.2203, "finish": 1717701353.4065, "state": {"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot", "tomato", "broccoli", "banana"], "output": null}, "ip": null}
{"tstamp": 1717701353.4065, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701353.2203, "finish": 1717701353.4065, "state": {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot", "tomato", "broccoli", "banana"], "output": null}, "ip": null}
{"tstamp": 1717701380.9042, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701380.663, "finish": 1717701380.9042, "state": {"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot", "tomato", "broccoli", "banana", "eggplant"], "output": null}, "ip": null}
{"tstamp": 1717701380.9042, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701380.663, "finish": 1717701380.9042, "state": {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot", "tomato", "broccoli", "banana", "eggplant"], "output": null}, "ip": null}
{"tstamp": 1717701455.8414, "task_type": "clustering", "type": "rightvote", "models": ["", ""], "states": [{"conv_id": "d5603c74fe544cd6877a6d6deecf65b2", "model_name": "intfloat/multilingual-e5-small", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot", "tomato", "broccoli", "banana", "eggplant"], "output": null}, {"conv_id": "adab481657454d3a825796a35f16fd1e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["pineapple", "apple", "mango", "peach", "lemon", "carrot", "tomato", "broccoli", "banana", "eggplant"], "output": null}], "ip": null}
{"tstamp": 1717701470.4423, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701470.0269, "finish": 1717701470.4423, "state": {"conv_id": "cc6a4a9bd51d425eb1d39d1cef3ca08a", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["darth vader"], "output": null}, "ip": null}
{"tstamp": 1717701470.4423, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701470.0269, "finish": 1717701470.4423, "state": {"conv_id": "9296e60ad8a447c1b2e21b322575fe7d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["darth vader"], "output": null}, "ip": null}
{"tstamp": 1717701473.4216, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701473.2569, "finish": 1717701473.4216, "state": {"conv_id": "cc6a4a9bd51d425eb1d39d1cef3ca08a", "model_name": "intfloat/multilingual-e5-small", "prompt": ["darth vader", "ahsoka tano"], "output": null}, "ip": null}
{"tstamp": 1717701473.4216, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701473.2569, "finish": 1717701473.4216, "state": {"conv_id": "9296e60ad8a447c1b2e21b322575fe7d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["darth vader", "ahsoka tano"], "output": null}, "ip": null}
{"tstamp": 1717701476.528, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701476.3832, "finish": 1717701476.528, "state": {"conv_id": "cc6a4a9bd51d425eb1d39d1cef3ca08a", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["darth vader", "ahsoka tano", "ben kenobi"], "output": null}, "ip": null}
{"tstamp": 1717701476.528, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701476.3832, "finish": 1717701476.528, "state": {"conv_id": "9296e60ad8a447c1b2e21b322575fe7d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["darth vader", "ahsoka tano", "ben kenobi"], "output": null}, "ip": null}
{"tstamp": 1717701479.6441, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701479.5119, "finish": 1717701479.6441, "state": {"conv_id": "cc6a4a9bd51d425eb1d39d1cef3ca08a", "model_name": "intfloat/multilingual-e5-small", "prompt": ["darth vader", "ahsoka tano", "ben kenobi", "pikachu"], "output": null}, "ip": null}
{"tstamp": 1717701479.6441, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701479.5119, "finish": 1717701479.6441, "state": {"conv_id": "9296e60ad8a447c1b2e21b322575fe7d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["darth vader", "ahsoka tano", "ben kenobi", "pikachu"], "output": null}, "ip": null}
{"tstamp": 1717701496.6159, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701496.3422, "finish": 1717701496.6159, "state": {"conv_id": "cc6a4a9bd51d425eb1d39d1cef3ca08a", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["darth vader", "ahsoka tano", "ben kenobi", "pikachu", "squirtle"], "output": null}, "ip": null}
{"tstamp": 1717701496.6159, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701496.3422, "finish": 1717701496.6159, "state": {"conv_id": "9296e60ad8a447c1b2e21b322575fe7d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["darth vader", "ahsoka tano", "ben kenobi", "pikachu", "squirtle"], "output": null}, "ip": null}
{"tstamp": 1717701504.0693, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701503.3534, "finish": 1717701504.0693, "state": {"conv_id": "cc6a4a9bd51d425eb1d39d1cef3ca08a", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["darth vader", "ahsoka tano", "ben kenobi", "pikachu", "squirtle", "bulbasaur"], "output": null}, "ip": null}
{"tstamp": 1717701504.0693, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701503.3534, "finish": 1717701504.0693, "state": {"conv_id": "9296e60ad8a447c1b2e21b322575fe7d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["darth vader", "ahsoka tano", "ben kenobi", "pikachu", "squirtle", "bulbasaur"], "output": null}, "ip": null}
{"tstamp": 1717701513.346, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701513.0524, "finish": 1717701513.346, "state": {"conv_id": "cc6a4a9bd51d425eb1d39d1cef3ca08a", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["darth vader", "ahsoka tano", "ben kenobi", "pikachu", "squirtle", "bulbasaur", "meowth"], "output": null}, "ip": null}
{"tstamp": 1717701513.346, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701513.0524, "finish": 1717701513.346, "state": {"conv_id": "9296e60ad8a447c1b2e21b322575fe7d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["darth vader", "ahsoka tano", "ben kenobi", "pikachu", "squirtle", "bulbasaur", "meowth"], "output": null}, "ip": null}
{"tstamp": 1717701548.4854, "task_type": "clustering", "type": "rightvote", "models": ["", ""], "states": [{"conv_id": "cc6a4a9bd51d425eb1d39d1cef3ca08a", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["darth vader", "ahsoka tano", "ben kenobi", "pikachu", "squirtle", "bulbasaur", "meowth"], "output": null}, {"conv_id": "9296e60ad8a447c1b2e21b322575fe7d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["darth vader", "ahsoka tano", "ben kenobi", "pikachu", "squirtle", "bulbasaur", "meowth"], "output": null}], "ip": null}
{"tstamp": 1717701575.8015, "task_type": "sts", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701575.3775, "finish": 1717701575.8015, "state": {"conv_id": "0f7e9870046b42d5a1b1fa7f91dd8dc5", "model_name": "intfloat/multilingual-e5-small", "txt0": "darth vader", "txt1": "darth maul", "txt2": "mace windu", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"66.31972715578857\" y2=\"188.68411112221867\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"66.31972715578857\" y1=\"188.68411112221867\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"66.31972715578857\" cy=\"188.68411112221867\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"71.31972715578857\" y=\"208.68411112221867\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, "ip": null}
{"tstamp": 1717701575.8015, "task_type": "sts", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701575.3775, "finish": 1717701575.8015, "state": {"conv_id": "3709132814654e549bcda00422b487f2", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "txt0": "darth vader", "txt1": "darth maul", "txt2": "mace windu", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"38.715674556429725\" y2=\"190.3791781023507\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"38.715674556429725\" y1=\"190.3791781023507\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"38.715674556429725\" cy=\"190.3791781023507\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"43.715674556429725\" y=\"210.3791781023507\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, "ip": null}
{"tstamp": 1717701582.5744, "task_type": "sts", "type": "rightvote", "models": ["", ""], "states": [{"conv_id": "0f7e9870046b42d5a1b1fa7f91dd8dc5", "model_name": "intfloat/multilingual-e5-small", "txt0": "darth vader", "txt1": "darth maul", "txt2": "mace windu", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"66.31972715578857\" y2=\"188.68411112221867\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"66.31972715578857\" y1=\"188.68411112221867\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"66.31972715578857\" cy=\"188.68411112221867\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"71.31972715578857\" y=\"208.68411112221867\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, {"conv_id": "3709132814654e549bcda00422b487f2", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "txt0": "darth vader", "txt1": "darth maul", "txt2": "mace windu", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"38.715674556429725\" y2=\"190.3791781023507\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"38.715674556429725\" y1=\"190.3791781023507\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"38.715674556429725\" cy=\"190.3791781023507\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"43.715674556429725\" y=\"210.3791781023507\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}], "ip": null}
{"tstamp": 1717701591.5384, "task_type": "sts", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701591.451, "finish": 1717701591.5384, "state": {"conv_id": "83bcf10acaf14dc08c1d04642e5884ab", "model_name": "intfloat/multilingual-e5-small", "txt0": "ahsoka tano", "txt1": "ahsoka", "txt2": "tano", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"-59.659725232644725\" y2=\"120.45236460375692\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"-59.659725232644725\" y1=\"120.45236460375692\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"-59.659725232644725\" cy=\"120.45236460375692\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"-54.659725232644725\" y=\"140.4523646037569\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, "ip": null}
{"tstamp": 1717701591.5384, "task_type": "sts", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701591.451, "finish": 1717701591.5384, "state": {"conv_id": "634b11c36d174a5d82d8ecb84297370c", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "txt0": "ahsoka tano", "txt1": "ahsoka", "txt2": "tano", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"-80.63453392509574\" y2=\"85.85548994481037\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"-80.63453392509574\" y1=\"85.85548994481037\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"-80.63453392509574\" cy=\"85.85548994481037\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"-75.63453392509574\" y=\"105.85548994481037\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, "ip": null}
{"tstamp": 1717701600.7903, "task_type": "sts", "type": "tievote", "models": ["", ""], "states": [{"conv_id": "83bcf10acaf14dc08c1d04642e5884ab", "model_name": "intfloat/multilingual-e5-small", "txt0": "ahsoka tano", "txt1": "ahsoka", "txt2": "tano", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"-59.659725232644725\" y2=\"120.45236460375692\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"-59.659725232644725\" y1=\"120.45236460375692\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"-59.659725232644725\" cy=\"120.45236460375692\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"-54.659725232644725\" y=\"140.4523646037569\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, {"conv_id": "634b11c36d174a5d82d8ecb84297370c", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "txt0": "ahsoka tano", "txt1": "ahsoka", "txt2": "tano", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"-80.63453392509574\" y2=\"85.85548994481037\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"-80.63453392509574\" y1=\"85.85548994481037\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"-80.63453392509574\" cy=\"85.85548994481037\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"-75.63453392509574\" y=\"105.85548994481037\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}], "ip": null}
{"tstamp": 1717701626.9859, "task_type": "sts", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701626.6318, "finish": 1717701626.9859, "state": {"conv_id": "ccf727e05f8a4e6bab17dbdf1ceedba0", "model_name": "intfloat/multilingual-e5-small", "txt0": "\u604b", "txt1": "\u2764\ufe0f", "txt2": "love", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"200.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"200.0\" y1=\"0\" x2=\"146.32782646436303\" y2=\"84.37593133092203\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"146.32782646436303\" y1=\"84.37593133092203\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"200.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"146.32782646436303\" cy=\"84.37593133092203\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"205.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"151.32782646436303\" y=\"104.37593133092203\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, "ip": null}
{"tstamp": 1717701626.9859, "task_type": "sts", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701626.6318, "finish": 1717701626.9859, "state": {"conv_id": "790faab88e3547dcbad8d1153205db79", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "txt0": "\u604b", "txt1": "\u2764\ufe0f", "txt2": "love", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"50.0\" y2=\"193.64916731037084\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"50.0\" y1=\"193.64916731037084\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"50.0\" cy=\"193.64916731037084\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"55.0\" y=\"213.64916731037084\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, "ip": null}
{"tstamp": 1717701634.4961, "task_type": "sts", "type": "leftvote", "models": ["", ""], "states": [{"conv_id": "ccf727e05f8a4e6bab17dbdf1ceedba0", "model_name": "intfloat/multilingual-e5-small", "txt0": "\u604b", "txt1": "\u2764\ufe0f", "txt2": "love", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"200.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"200.0\" y1=\"0\" x2=\"146.32782646436303\" y2=\"84.37593133092203\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"146.32782646436303\" y1=\"84.37593133092203\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"200.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"146.32782646436303\" cy=\"84.37593133092203\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"205.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"151.32782646436303\" y=\"104.37593133092203\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, {"conv_id": "790faab88e3547dcbad8d1153205db79", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "txt0": "\u604b", "txt1": "\u2764\ufe0f", "txt2": "love", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"50.0\" y2=\"193.64916731037084\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"50.0\" y1=\"193.64916731037084\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"50.0\" cy=\"193.64916731037084\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"55.0\" y=\"213.64916731037084\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}], "ip": null}
{"tstamp": 1717701658.8561, "task_type": "sts", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701658.4337, "finish": 1717701658.8561, "state": {"conv_id": "c59da4ce393d4bf4b374fe42b4ba474e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "txt0": "\u65e9\u4e0a\u597d", "txt1": "good morning", "txt2": "hello", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"200.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"200.0\" y1=\"0\" x2=\"153.15938882842252\" y2=\"88.3513279191269\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"153.15938882842252\" y1=\"88.3513279191269\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"200.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"153.15938882842252\" cy=\"88.3513279191269\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"205.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"158.15938882842252\" y=\"108.3513279191269\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, "ip": null}
{"tstamp": 1717701658.8561, "task_type": "sts", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701658.4337, "finish": 1717701658.8561, "state": {"conv_id": "bf8740b661d8428b99c75816bab93680", "model_name": "intfloat/multilingual-e5-small", "txt0": "\u65e9\u4e0a\u597d", "txt1": "good morning", "txt2": "hello", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"186.99905376274955\" y2=\"70.93203713299306\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"186.99905376274955\" y1=\"70.93203713299306\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"186.99905376274955\" cy=\"70.93203713299306\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"191.99905376274955\" y=\"90.93203713299306\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, "ip": null}
{"tstamp": 1717701665.9885, "task_type": "sts", "type": "rightvote", "models": ["", ""], "states": [{"conv_id": "c59da4ce393d4bf4b374fe42b4ba474e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "txt0": "\u65e9\u4e0a\u597d", "txt1": "good morning", "txt2": "hello", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"200.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"200.0\" y1=\"0\" x2=\"153.15938882842252\" y2=\"88.3513279191269\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"153.15938882842252\" y1=\"88.3513279191269\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"200.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"153.15938882842252\" cy=\"88.3513279191269\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"205.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"158.15938882842252\" y=\"108.3513279191269\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, {"conv_id": "bf8740b661d8428b99c75816bab93680", "model_name": "intfloat/multilingual-e5-small", "txt0": "\u65e9\u4e0a\u597d", "txt1": "good morning", "txt2": "hello", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"186.99905376274955\" y2=\"70.93203713299306\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"186.99905376274955\" y1=\"70.93203713299306\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"186.99905376274955\" cy=\"70.93203713299306\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"191.99905376274955\" y=\"90.93203713299306\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}], "ip": null}
{"tstamp": 1717701802.1625, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701796.4675, "finish": 1717701802.1625, "state": {"conv_id": "d6627d18db334c4ea55caaf7e460f424", "model_name": "intfloat/multilingual-e5-small", "prompt": "What is \u7aef\u5348\u8282\uff1f", "output": [["What is \u7aef\u5348\u8282\uff1f", "Title: Dragon Boat Festival\n\nPassage: The official Chinese name of the festival is \u7aef\u5348\u8282 on the mainland,[2] Malaysia and Singapore,[3] and \u7aef\u5348\u7bc0 in Taiwan,[4] Hong Kong and Macao.[5] This is pronounced variously in different Chinese languages. In Mandarin, it is romanized as Du\u0101nw\u01d4ji\u00e9 on the mainland and Taiwan; in Cantonese, it is romanized as Tuen1 Ng5 Jit3 on Hong Kong and Tung1 Ng5 Jit3 on Macao. All of these names (lit.\u00a0\"Opening the Seventh\") refer to its original position as the first seventh-day (\u5348\u65e5, W\u01d4r\u00ec) in the fifth month (\u4e94\u6708, W\u01d4yu\u00e8) of the traditional Chinese calendar, which was also known as \u5348 (W\u01d4). People's Republic of China use \"Dragon Boat Festival\" as the official English translation of the holiday,[6][7] while Hong Kong calls it the \"Tuen Ng Festival\"[3] and Macao calls it \"Dragon Boat Festival (Tun Ng)\" in English[8] and Festividade do Barco-Drag\u00e3o (Tung Ng) in Portuguese.[9]"]]}, "ip": null}
{"tstamp": 1717701802.1625, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701796.4675, "finish": 1717701802.1625, "state": {"conv_id": "f299e4fd0ca14988a4b4807abf3a65a2", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "What is \u7aef\u5348\u8282\uff1f", "output": [["What is \u7aef\u5348\u8282\uff1f", "Title: Atomic bombings of Hiroshima and Nagasaki\n\nPassage: Unlike the other target cities, Nagasaki had not been placed off limits to bombers by the Joint Chiefs of Staff's July 3 directive,[119][186] and was bombed on a small scale five times. During one of these raids on August 1, a number of conventional high-explosive bombs were dropped on the city. A few hit the shipyards and dock areas in the southwest portion of the city, and several hit the Mitsubishi Steel and Arms Works.[185] By early August, the city was defended by the 134th Anti-Aircraft Regiment of the 4th Anti-Aircraft Division with four batteries of 7\u00a0cm (2.8\u00a0in) anti-aircraft guns and two searchlight batteries.[114]"]]}, "ip": null}
{"tstamp": 1717701863.2943, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701862.8922, "finish": 1717701863.2943, "state": {"conv_id": "b5ab49cc2f5141cfb98cd7571b3611d8", "model_name": "intfloat/multilingual-e5-small", "prompt": "hello", "output": [["hello", "Title: Tobacco advertising\n\nPassage: Tobacco advertising is the advertising of tobacco products or use (typically cigarette smoking) by the tobacco industry through a variety of media including sponsorship, particularly of sporting events. It is now one of the most highly regulated forms of marketing. Some or all forms of tobacco advertising are banned in many countries."]]}, "ip": null}
{"tstamp": 1717701863.2943, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701862.8922, "finish": 1717701863.2943, "state": {"conv_id": "5799af4792e44e16aaf0205d8fded32d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "hello", "output": [["hello", "Title: Sperm\n\nPassage: Motile sperm cells typically move via flagella and require a water medium in order to swim toward the egg for fertilization. In animals most of the energy for sperm motility is derived from the metabolism of fructose carried in the seminal fluid. This takes place in the mitochondria located in the sperm's midpiece (at the base of the sperm head). These cells cannot swim backwards due to the nature of their propulsion. The uniflagellated sperm cells (with one flagellum) of animals are referred to as spermatozoa, and are known to vary in size.[citation needed]"]]}, "ip": null}
{"tstamp": 1717701878.8866, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717701878.6755, "finish": 1717701878.8866, "state": {"conv_id": "dfbe37d3768048c3ae205a76471ad44e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "What's a famous festival in China?", "output": [["What's a famous festival in China?", "Title: Patrick Brown (politician)\n\nPassage: In May 2015, Brown was elected leader of the Ontario PC Party, and stepped down as MP. He was elected Member of Provincial Parliament (MPP) for Simcoe North in a provincial by-election on September 3, 2015. Before being elected to federal office, Brown worked as a lawyer in Barrie.[1]"]]}, "ip": null}
{"tstamp": 1717701878.8866, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717701878.6755, "finish": 1717701878.8866, "state": {"conv_id": "995b788852dc4c0091c189624bddf93b", "model_name": "intfloat/multilingual-e5-small", "prompt": "What's a famous festival in China?", "output": [["What's a famous festival in China?", "Title: Dragon Boat Festival\n\nPassage: The Duanwu Festival, also often known, especially in the West, as the Dragon Boat Festival, is a traditional holiday originating in China, occurring near the summer solstice. It is also known as Zhongxiao Festival (Chinese: \u5fe0\u5b5d\u7bc0; pinyin: Zh\u014dngxi\u00e0oji\u00e9), commemorating fealty and filial piety. The festival now occurs on the 5th day of the 5th month of the traditional Chinese calendar, which is the source of the festival's alternative name, the Double Fifth Festival.[1] The Chinese calendar is lunisolar, so the date of the festival varies from year to year on the Gregorian calendar. In 2016, it occurred on June 9; and in 2017, on May 30."]]}, "ip": null}
{"tstamp": 1717702343.4222, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702342.8895, "finish": 1717702343.4222, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter"], "output": null}, "ip": null}
{"tstamp": 1717702343.4222, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702342.8895, "finish": 1717702343.4222, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter"], "output": null}, "ip": null}
{"tstamp": 1717702346.6976, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702346.4527, "finish": 1717702346.6976, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus"], "output": null}, "ip": null}
{"tstamp": 1717702346.6976, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702346.4527, "finish": 1717702346.6976, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus"], "output": null}, "ip": null}
{"tstamp": 1717702351.813, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702351.6245, "finish": 1717702351.813, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune"], "output": null}, "ip": null}
{"tstamp": 1717702351.813, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702351.6245, "finish": 1717702351.813, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune"], "output": null}, "ip": null}
{"tstamp": 1717702362.7697, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702362.5718, "finish": 1717702362.7697, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn"], "output": null}, "ip": null}
{"tstamp": 1717702362.7697, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702362.5718, "finish": 1717702362.7697, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn"], "output": null}, "ip": null}
{"tstamp": 1717702385.5346, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702385.3025, "finish": 1717702385.5346, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto"], "output": null}, "ip": null}
{"tstamp": 1717702385.5346, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702385.3025, "finish": 1717702385.5346, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto"], "output": null}, "ip": null}
{"tstamp": 1717702388.7036, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702388.5138, "finish": 1717702388.7036, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka"], "output": null}, "ip": null}
{"tstamp": 1717702388.7036, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702388.5138, "finish": 1717702388.7036, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka"], "output": null}, "ip": null}
{"tstamp": 1717702406.1029, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702405.84, "finish": 1717702406.1029, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin"], "output": null}, "ip": null}
{"tstamp": 1717702406.1029, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702405.84, "finish": 1717702406.1029, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin"], "output": null}, "ip": null}
{"tstamp": 1717702416.7378, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702416.5619, "finish": 1717702416.7378, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo"], "output": null}, "ip": null}
{"tstamp": 1717702416.7378, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702416.5619, "finish": 1717702416.7378, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo"], "output": null}, "ip": null}
{"tstamp": 1717702432.6182, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702432.4294, "finish": 1717702432.6182, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka"], "output": null}, "ip": null}
{"tstamp": 1717702432.6182, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702432.4294, "finish": 1717702432.6182, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka"], "output": null}, "ip": null}
{"tstamp": 1717702475.9839, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702475.5259, "finish": 1717702475.9839, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin"], "output": null}, "ip": null}
{"tstamp": 1717702475.9839, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702475.5259, "finish": 1717702475.9839, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin"], "output": null}, "ip": null}
{"tstamp": 1717702478.5329, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702478.3338, "finish": 1717702478.5329, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain"], "output": null}, "ip": null}
{"tstamp": 1717702478.5329, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702478.3338, "finish": 1717702478.5329, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain"], "output": null}, "ip": null}
{"tstamp": 1717702499.2756, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702498.818, "finish": 1717702499.2756, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer"], "output": null}, "ip": null}
{"tstamp": 1717702499.2756, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702498.818, "finish": 1717702499.2756, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer"], "output": null}, "ip": null}
{"tstamp": 1717702522.1985, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702521.5745, "finish": 1717702522.1985, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work"], "output": null}, "ip": null}
{"tstamp": 1717702522.1985, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702521.5745, "finish": 1717702522.1985, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work"], "output": null}, "ip": null}
{"tstamp": 1717702590.1471, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702589.6065, "finish": 1717702590.1471, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256"], "output": null}, "ip": null}
{"tstamp": 1717702590.1471, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702589.6065, "finish": 1717702590.1471, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256"], "output": null}, "ip": null}
{"tstamp": 1717702661.7684, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702661.2981, "finish": 1717702661.7684, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256", "mars"], "output": null}, "ip": null}
{"tstamp": 1717702661.7684, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702661.2981, "finish": 1717702661.7684, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256", "mars"], "output": null}, "ip": null}
{"tstamp": 1717702693.2918, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702693.066, "finish": 1717702693.2918, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256", "mars", "nagoya"], "output": null}, "ip": null}
{"tstamp": 1717702693.2918, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702693.066, "finish": 1717702693.2918, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256", "mars", "nagoya"], "output": null}, "ip": null}
{"tstamp": 1717702745.8152, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702745.2906, "finish": 1717702745.8152, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256", "mars", "nagoya", "machine learning"], "output": null}, "ip": null}
{"tstamp": 1717702745.8152, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702745.2906, "finish": 1717702745.8152, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256", "mars", "nagoya", "machine learning"], "output": null}, "ip": null}
{"tstamp": 1717702753.0001, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702752.8017, "finish": 1717702753.0001, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256", "mars", "nagoya", "machine learning", "natural language processing"], "output": null}, "ip": null}
{"tstamp": 1717702753.0001, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702752.8017, "finish": 1717702753.0001, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256", "mars", "nagoya", "machine learning", "natural language processing"], "output": null}, "ip": null}
{"tstamp": 1717702756.8361, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702756.6448, "finish": 1717702756.8361, "state": {"conv_id": "bcb11e39b91e4a2e802db605a94f31fe", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256", "mars", "nagoya", "machine learning", "natural language processing", "artificial intelligence"], "output": null}, "ip": null}
{"tstamp": 1717702756.8361, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702756.6448, "finish": 1717702756.8361, "state": {"conv_id": "6daf9570b7cd466c9b54e75bec30d3a6", "model_name": "intfloat/multilingual-e5-small", "prompt": ["jupiter", "uranus", "neptune", "saturn", "kyoto", "osaka", "yufuin", "sapporo", "fukuoka", "bitcoin", "blockchain", "peer-to-peer", "proof of work", "SHA-256", "mars", "nagoya", "machine learning", "natural language processing", "artificial intelligence"], "output": null}, "ip": null}
{"tstamp": 1717702838.2251, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702837.8146, "finish": 1717702838.2251, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo"], "output": null}, "ip": null}
{"tstamp": 1717702838.2251, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702837.8146, "finish": 1717702838.2251, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo"], "output": null}, "ip": null}
{"tstamp": 1717702840.7537, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702840.5651, "finish": 1717702840.7537, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto"], "output": null}, "ip": null}
{"tstamp": 1717702840.7537, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702840.5651, "finish": 1717702840.7537, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto"], "output": null}, "ip": null}
{"tstamp": 1717702843.0397, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702842.8743, "finish": 1717702843.0397, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka"], "output": null}, "ip": null}
{"tstamp": 1717702843.0397, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702842.8743, "finish": 1717702843.0397, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka"], "output": null}, "ip": null}
{"tstamp": 1717702847.9264, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702847.7772, "finish": 1717702847.9264, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima"], "output": null}, "ip": null}
{"tstamp": 1717702847.9264, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702847.7772, "finish": 1717702847.9264, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima"], "output": null}, "ip": null}
{"tstamp": 1717702853.2873, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702853.1391, "finish": 1717702853.2873, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo"], "output": null}, "ip": null}
{"tstamp": 1717702853.2873, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702853.1391, "finish": 1717702853.2873, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo"], "output": null}, "ip": null}
{"tstamp": 1717702863.687, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702863.4557, "finish": 1717702863.687, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter"], "output": null}, "ip": null}
{"tstamp": 1717702863.687, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702863.4557, "finish": 1717702863.687, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter"], "output": null}, "ip": null}
{"tstamp": 1717702865.4599, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702865.3026, "finish": 1717702865.4599, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn"], "output": null}, "ip": null}
{"tstamp": 1717702865.4599, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702865.3026, "finish": 1717702865.4599, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn"], "output": null}, "ip": null}
{"tstamp": 1717702867.1341, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702866.9969, "finish": 1717702867.1341, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars"], "output": null}, "ip": null}
{"tstamp": 1717702867.1341, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702866.9969, "finish": 1717702867.1341, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars"], "output": null}, "ip": null}
{"tstamp": 1717702869.2272, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702869.0976, "finish": 1717702869.2272, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune"], "output": null}, "ip": null}
{"tstamp": 1717702869.2272, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702869.0976, "finish": 1717702869.2272, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune"], "output": null}, "ip": null}
{"tstamp": 1717702872.2923, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702872.1564, "finish": 1717702872.2923, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain"], "output": null}, "ip": null}
{"tstamp": 1717702872.2923, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702872.1564, "finish": 1717702872.2923, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain"], "output": null}, "ip": null}
{"tstamp": 1717702874.1288, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702873.9999, "finish": 1717702874.1288, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin"], "output": null}, "ip": null}
{"tstamp": 1717702874.1288, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702873.9999, "finish": 1717702874.1288, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin"], "output": null}, "ip": null}
{"tstamp": 1717702877.9346, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702877.7746, "finish": 1717702877.9346, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work"], "output": null}, "ip": null}
{"tstamp": 1717702877.9346, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702877.7746, "finish": 1717702877.9346, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work"], "output": null}, "ip": null}
{"tstamp": 1717702892.5575, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702892.2238, "finish": 1717702892.5575, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency"], "output": null}, "ip": null}
{"tstamp": 1717702892.5575, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702892.2238, "finish": 1717702892.5575, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency"], "output": null}, "ip": null}
{"tstamp": 1717702947.8522, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717702947.3802, "finish": 1717702947.8522, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus"], "output": null}, "ip": null}
{"tstamp": 1717702947.8522, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717702947.3802, "finish": 1717702947.8522, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus"], "output": null}, "ip": null}
{"tstamp": 1717703065.293, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717703064.7727, "finish": 1717703065.293, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus"], "output": null}, "ip": null}
{"tstamp": 1717703065.293, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717703064.7727, "finish": 1717703065.293, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus"], "output": null}, "ip": null}
{"tstamp": 1717703197.557, "task_type": "sts", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717703197.1304, "finish": 1717703197.557, "state": {"conv_id": "53a18ddb60e64e71936b25cce6f9df8f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "txt0": "hello", "txt1": "good morning", "txt2": "\u65e9\u4e0a\u597d", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"6.318777656845041\" y2=\"176.70265583825378\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"6.318777656845041\" y1=\"176.70265583825378\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"6.318777656845041\" cy=\"176.70265583825378\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"11.318777656845041\" y=\"196.70265583825378\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, "ip": null}
{"tstamp": 1717703197.557, "task_type": "sts", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717703197.1304, "finish": 1717703197.557, "state": {"conv_id": "17311d02e2c645edb5caabb181aecc64", "model_name": "intfloat/multilingual-e5-small", "txt0": "hello", "txt1": "good morning", "txt2": "\u65e9\u4e0a\u597d", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"112.25056457519531\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"112.25056457519531\" y1=\"0\" x2=\"189.7548997132782\" y2=\"63.19080656870687\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"189.7548997132782\" y1=\"63.19080656870687\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"112.25056457519531\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"189.7548997132782\" cy=\"63.19080656870687\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"117.25056457519531\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"194.7548997132782\" y=\"83.19080656870688\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, "ip": null}
{"tstamp": 1717703231.007, "task_type": "sts", "type": "rightvote", "models": ["", ""], "states": [{"conv_id": "53a18ddb60e64e71936b25cce6f9df8f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "txt0": "hello", "txt1": "good morning", "txt2": "\u65e9\u4e0a\u597d", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"100.0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"100.0\" y1=\"0\" x2=\"6.318777656845041\" y2=\"176.70265583825378\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"6.318777656845041\" y1=\"176.70265583825378\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"100.0\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"6.318777656845041\" cy=\"176.70265583825378\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"105.0\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"11.318777656845041\" y=\"196.70265583825378\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}, {"conv_id": "17311d02e2c645edb5caabb181aecc64", "model_name": "intfloat/multilingual-e5-small", "txt0": "hello", "txt1": "good morning", "txt2": "\u65e9\u4e0a\u597d", "output": "\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Cosine Similarity Triangle</title>\n</head>\n<body>\n    <svg width=\"400\" height=\"400\" viewBox=\"-150 -50 400 400\">\n        <!-- Draw the triangle -->\n        <line x1=\"0\" y1=\"0\" x2=\"112.25056457519531\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"112.25056457519531\" y1=\"0\" x2=\"189.7548997132782\" y2=\"63.19080656870687\" stroke=\"black\" stroke-width=\"2\"/>\n        <line x1=\"189.7548997132782\" y1=\"63.19080656870687\" x2=\"0\" y2=\"0\" stroke=\"black\" stroke-width=\"2\"/>\n        \n        <!-- Draw the points -->\n        <circle cx=\"0\" cy=\"0\" r=\"8\" fill=\"red\"/>\n        <circle cx=\"112.25056457519531\" cy=\"0\" r=\"8\" fill=\"green\"/>\n        <circle cx=\"189.7548997132782\" cy=\"63.19080656870687\" r=\"8\" fill=\"blue\"/>\n        \n        <!-- Label the points -->\n        <text x=\"5\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(1)</text>\n        <text x=\"117.25056457519531\" y=\"-5\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(2)</text>\n        <text x=\"194.7548997132782\" y=\"83.19080656870688\" font-family=\"Arial\" font-size=\"40\" fill=\"black\">(3)</text>\n    </svg>\n</body>\n</html>\n"}], "ip": null}
{"tstamp": 1717703682.0571, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717703681.6162, "finish": 1717703682.0571, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake"], "output": null}, "ip": null}
{"tstamp": 1717703682.0571, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717703681.6162, "finish": 1717703682.0571, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake"], "output": null}, "ip": null}
{"tstamp": 1717703713.0502, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717703712.7399, "finish": 1717703713.0502, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum"], "output": null}, "ip": null}
{"tstamp": 1717703713.0502, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717703712.7399, "finish": 1717703713.0502, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum"], "output": null}, "ip": null}
{"tstamp": 1717706464.3737, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717706463.9044, "finish": 1717706464.3737, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC"], "output": null}, "ip": null}
{"tstamp": 1717706464.3737, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717706463.9044, "finish": 1717706464.3737, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC"], "output": null}, "ip": null}
{"tstamp": 1717706608.4853, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717706607.9556, "finish": 1717706608.4853, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH"], "output": null}, "ip": null}
{"tstamp": 1717706608.4853, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717706607.9556, "finish": 1717706608.4853, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH"], "output": null}, "ip": null}
{"tstamp": 1717706745.3202, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717706744.7909, "finish": 1717706745.3202, "state": {"conv_id": "cb847088e91d42259a54168138c48ebb", "model_name": "intfloat/multilingual-e5-small", "prompt": ["\u82f9\u679c"], "output": null}, "ip": null}
{"tstamp": 1717706745.3202, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717706744.7909, "finish": 1717706745.3202, "state": {"conv_id": "5f461ea6b5ea4cd0ac78a1a185d11051", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["\u82f9\u679c"], "output": null}, "ip": null}
{"tstamp": 1717706748.044, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717706747.91, "finish": 1717706748.044, "state": {"conv_id": "cb847088e91d42259a54168138c48ebb", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["\u82f9\u679c", "apple"], "output": null}, "ip": null}
{"tstamp": 1717706748.044, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717706747.91, "finish": 1717706748.044, "state": {"conv_id": "5f461ea6b5ea4cd0ac78a1a185d11051", "model_name": "intfloat/multilingual-e5-small", "prompt": ["\u82f9\u679c", "apple"], "output": null}, "ip": null}
{"tstamp": 1717706893.0199, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717706892.5144, "finish": 1717706893.0199, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b"], "output": null}, "ip": null}
{"tstamp": 1717706893.0199, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717706892.5144, "finish": 1717706893.0199, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b"], "output": null}, "ip": null}
{"tstamp": 1717706950.3594, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717706949.8329, "finish": 1717706950.3594, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b"], "output": null}, "ip": null}
{"tstamp": 1717706950.3594, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717706949.8329, "finish": 1717706950.3594, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b"], "output": null}, "ip": null}
{"tstamp": 1717707049.389, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717707048.8706, "finish": 1717707049.389, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b"], "output": null}, "ip": null}
{"tstamp": 1717707049.389, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717707048.8706, "finish": 1717707049.389, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b"], "output": null}, "ip": null}
{"tstamp": 1717707077.6419, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717707077.089, "finish": 1717707077.6419, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d"], "output": null}, "ip": null}
{"tstamp": 1717707077.6419, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717707077.089, "finish": 1717707077.6419, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d"], "output": null}, "ip": null}
{"tstamp": 1717707114.6965, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717707114.3329, "finish": 1717707114.6965, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai"], "output": null}, "ip": null}
{"tstamp": 1717707114.6965, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717707114.3329, "finish": 1717707114.6965, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai"], "output": null}, "ip": null}
{"tstamp": 1717707117.0929, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717707116.838, "finish": 1717707117.0929, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai", "beijing"], "output": null}, "ip": null}
{"tstamp": 1717707117.0929, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717707116.838, "finish": 1717707117.0929, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai", "beijing"], "output": null}, "ip": null}
{"tstamp": 1717707119.2097, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717707118.9337, "finish": 1717707119.2097, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai", "beijing", "shenzhen"], "output": null}, "ip": null}
{"tstamp": 1717707119.2097, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717707118.9337, "finish": 1717707119.2097, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai", "beijing", "shenzhen"], "output": null}, "ip": null}
{"tstamp": 1717707136.5978, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717707136.2218, "finish": 1717707136.5978, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai", "beijing", "shenzhen", "hangzhou"], "output": null}, "ip": null}
{"tstamp": 1717707136.5978, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717707136.2218, "finish": 1717707136.5978, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai", "beijing", "shenzhen", "hangzhou"], "output": null}, "ip": null}
{"tstamp": 1717707320.4792, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717707319.923, "finish": 1717707320.4792, "state": {"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai", "beijing", "shenzhen", "hangzhou", "guangzhou"], "output": null}, "ip": null}
{"tstamp": 1717707320.4792, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717707319.923, "finish": 1717707320.4792, "state": {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai", "beijing", "shenzhen", "hangzhou", "guangzhou"], "output": null}, "ip": null}
{"tstamp": 1717707438.6238, "task_type": "clustering", "type": "tievote", "models": ["", ""], "states": [{"conv_id": "05001e9c8fef41dc956b84803830b38e", "model_name": "intfloat/multilingual-e5-small", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai", "beijing", "shenzhen", "hangzhou", "guangzhou"], "output": null}, {"conv_id": "264bd3f8bd794ca588e808381f3e8630", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["tokyo", "kyoto", "osaka", "hiroshima", "sapporo", "jupiter", "saturn", "mars", "neptune", "blockchain", "bitcoin", "proof of work", "cryptocurrency", "uranus", "venus", "proof of stake", "ethereum", "BTC", "ETH", "TRAPPIST-1 b", "Kepler-22 b", "Proxima Centauri b", "TOI-700 d", "shanghai", "beijing", "shenzhen", "hangzhou", "guangzhou"], "output": null}], "ip": null}
{"tstamp": 1717707473.669, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717707473.1709, "finish": 1717707473.669, "state": {"conv_id": "9aaf56a1d0a94df2aa2a9f04b70a7fff", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["beppu"], "output": null}, "ip": null}
{"tstamp": 1717707473.669, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717707473.1709, "finish": 1717707473.669, "state": {"conv_id": "215ec88519c948f49cae9c91db738e18", "model_name": "intfloat/multilingual-e5-small", "prompt": ["beppu"], "output": null}, "ip": null}
{"tstamp": 1717707508.1049, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717707507.6727, "finish": 1717707508.1049, "state": {"conv_id": "9aaf56a1d0a94df2aa2a9f04b70a7fff", "model_name": "intfloat/multilingual-e5-small", "prompt": ["beppu", "kushima"], "output": null}, "ip": null}
{"tstamp": 1717707508.1049, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717707507.6727, "finish": 1717707508.1049, "state": {"conv_id": "215ec88519c948f49cae9c91db738e18", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["beppu", "kushima"], "output": null}, "ip": null}
{"tstamp": 1717712214.6724, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712214.1078, "finish": 1717712214.6724, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power"], "output": null}, "ip": null}
{"tstamp": 1717712214.6724, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712214.1078, "finish": 1717712214.6724, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power"], "output": null}, "ip": null}
{"tstamp": 1717712223.1224, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712222.8852, "finish": 1717712223.1224, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions"], "output": null}, "ip": null}
{"tstamp": 1717712223.1224, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712222.8852, "finish": 1717712223.1224, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions"], "output": null}, "ip": null}
{"tstamp": 1717712229.8933, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712229.5745, "finish": 1717712229.8933, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis"], "output": null}, "ip": null}
{"tstamp": 1717712229.8933, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712229.5745, "finish": 1717712229.8933, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis"], "output": null}, "ip": null}
{"tstamp": 1717712241.7408, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712241.4323, "finish": 1717712241.7408, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium"], "output": null}, "ip": null}
{"tstamp": 1717712241.7408, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712241.4323, "finish": 1717712241.7408, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium"], "output": null}, "ip": null}
{"tstamp": 1717712251.2668, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712250.9983, "finish": 1717712251.2668, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments"], "output": null}, "ip": null}
{"tstamp": 1717712251.2668, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712250.9983, "finish": 1717712251.2668, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments"], "output": null}, "ip": null}
{"tstamp": 1717712271.0986, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712270.6561, "finish": 1717712271.0986, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models"], "output": null}, "ip": null}
{"tstamp": 1717712271.0986, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712270.6561, "finish": 1717712271.0986, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models"], "output": null}, "ip": null}
{"tstamp": 1717712285.3485, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712284.7557, "finish": 1717712285.3485, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models", "MTEB: Massive text embedding benchmark"], "output": null}, "ip": null}
{"tstamp": 1717712285.3485, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712284.7557, "finish": 1717712285.3485, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models", "MTEB: Massive text embedding benchmark"], "output": null}, "ip": null}
{"tstamp": 1717712292.4948, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712292.1249, "finish": 1717712292.4948, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models", "MTEB: Massive text embedding benchmark", "Crosslingual generalization through multitask finetuning"], "output": null}, "ip": null}
{"tstamp": 1717712292.4948, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712292.1249, "finish": 1717712292.4948, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models", "MTEB: Massive text embedding benchmark", "Crosslingual generalization through multitask finetuning"], "output": null}, "ip": null}
{"tstamp": 1717712311.5764, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712311.279, "finish": 1717712311.5764, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models", "MTEB: Massive text embedding benchmark", "Crosslingual generalization through multitask finetuning", "SGPT: GPT sentence embeddings for semantic search"], "output": null}, "ip": null}
{"tstamp": 1717712311.5764, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712311.279, "finish": 1717712311.5764, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models", "MTEB: Massive text embedding benchmark", "Crosslingual generalization through multitask finetuning", "SGPT: GPT sentence embeddings for semantic search"], "output": null}, "ip": null}
{"tstamp": 1717712324.0822, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712323.7624, "finish": 1717712324.0822, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models", "MTEB: Massive text embedding benchmark", "Crosslingual generalization through multitask finetuning", "SGPT: GPT sentence embeddings for semantic search", "Generative representational instruction tuning"], "output": null}, "ip": null}
{"tstamp": 1717712324.0822, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712323.7624, "finish": 1717712324.0822, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models", "MTEB: Massive text embedding benchmark", "Crosslingual generalization through multitask finetuning", "SGPT: GPT sentence embeddings for semantic search", "Generative representational instruction tuning"], "output": null}, "ip": null}
{"tstamp": 1717712429.7242, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712429.2581, "finish": 1717712429.7242, "state": {"conv_id": "fac528dbda844e7492856a5983c0c39b", "model_name": "intfloat/multilingual-e5-small", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models", "MTEB: Massive text embedding benchmark", "Crosslingual generalization through multitask finetuning", "SGPT: GPT sentence embeddings for semantic search", "Generative representational instruction tuning", "Octopack: Instruction tuning code large language models"], "output": null}, "ip": null}
{"tstamp": 1717712429.7242, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712429.2581, "finish": 1717712429.7242, "state": {"conv_id": "56a66faac47546b7b2f0cf551ec25997", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["PMCE: efficient inference of expressive models of cancer evolution with high prognostic power", "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "Fast gene set enrichment analysis", "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "Scaling Data-Constrained Language Models", "MTEB: Massive text embedding benchmark", "Crosslingual generalization through multitask finetuning", "SGPT: GPT sentence embeddings for semantic search", "Generative representational instruction tuning", "Octopack: Instruction tuning code large language models"], "output": null}, "ip": null}
{"tstamp": 1717712554.862, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712554.2741, "finish": 1717712554.862, "state": {"conv_id": "c3828723b5da498eb82c0a6aee56967c", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf."], "output": null}, "ip": null}
{"tstamp": 1717712554.862, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712554.2741, "finish": 1717712554.862, "state": {"conv_id": "388e2a90d03e4023a75dfadfec50eb46", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf."], "output": null}, "ip": null}
{"tstamp": 1717712572.8315, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712572.2143, "finish": 1717712572.8315, "state": {"conv_id": "c3828723b5da498eb82c0a6aee56967c", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb."], "output": null}, "ip": null}
{"tstamp": 1717712572.8315, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712572.2143, "finish": 1717712572.8315, "state": {"conv_id": "388e2a90d03e4023a75dfadfec50eb46", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb."], "output": null}, "ip": null}
{"tstamp": 1717712585.2796, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712584.9054, "finish": 1717712585.2796, "state": {"conv_id": "c3828723b5da498eb82c0a6aee56967c", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt."], "output": null}, "ip": null}
{"tstamp": 1717712585.2796, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712584.9054, "finish": 1717712585.2796, "state": {"conv_id": "388e2a90d03e4023a75dfadfec50eb46", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt."], "output": null}, "ip": null}
{"tstamp": 1717712666.3576, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712661.9321, "finish": 1717712666.3576, "state": {"conv_id": "2f9b82b3f04c4704b67419c5a547f58f", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "SGPT: GPT sentence embeddings for semantic search Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "output": [["SGPT: GPT sentence embeddings for semantic search Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Title: Tobacco advertising\n\nPassage: Tobacco advertising has been hand-in-hand with many sports internationally. Below are some sports."]]}, "ip": null}
{"tstamp": 1717712666.3576, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712661.9321, "finish": 1717712666.3576, "state": {"conv_id": "0af3091072084b28a689e8fdb2b41e93", "model_name": "intfloat/multilingual-e5-small", "prompt": "SGPT: GPT sentence embeddings for semantic search Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "output": [["SGPT: GPT sentence embeddings for semantic search Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Title: Multilayer perceptron\n\nPassage: Using gradient descent, the change in each weight is"]]}, "ip": null}
{"tstamp": 1717712675.0558, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712674.8356, "finish": 1717712675.0558, "state": {"conv_id": "d9c980523b334677b31aeed18a1649ed", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt."], "output": null}, "ip": null}
{"tstamp": 1717712675.0558, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712674.8356, "finish": 1717712675.0558, "state": {"conv_id": "2c7e6cf8b25f4f849d9b0f8e54a5af95", "model_name": "intfloat/multilingual-e5-small", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt."], "output": null}, "ip": null}
{"tstamp": 1717712696.1776, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712695.8899, "finish": 1717712696.1776, "state": {"conv_id": "d9c980523b334677b31aeed18a1649ed", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf."], "output": null}, "ip": null}
{"tstamp": 1717712696.1776, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712695.8899, "finish": 1717712696.1776, "state": {"conv_id": "2c7e6cf8b25f4f849d9b0f8e54a5af95", "model_name": "intfloat/multilingual-e5-small", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf."], "output": null}, "ip": null}
{"tstamp": 1717712713.9256, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712713.288, "finish": 1717712713.9256, "state": {"conv_id": "d9c980523b334677b31aeed18a1649ed", "model_name": "intfloat/multilingual-e5-small", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb."], "output": null}, "ip": null}
{"tstamp": 1717712713.9256, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712713.288, "finish": 1717712713.9256, "state": {"conv_id": "2c7e6cf8b25f4f849d9b0f8e54a5af95", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb."], "output": null}, "ip": null}
{"tstamp": 1717712736.158, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712735.5595, "finish": 1717712736.158, "state": {"conv_id": "d9c980523b334677b31aeed18a1649ed", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations."], "output": null}, "ip": null}
{"tstamp": 1717712736.158, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712735.5595, "finish": 1717712736.158, "state": {"conv_id": "2c7e6cf8b25f4f849d9b0f8e54a5af95", "model_name": "intfloat/multilingual-e5-small", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations."], "output": null}, "ip": null}
{"tstamp": 1717712762.0426, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712761.4316, "finish": 1717712762.0426, "state": {"conv_id": "d9c980523b334677b31aeed18a1649ed", "model_name": "intfloat/multilingual-e5-small", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack."], "output": null}, "ip": null}
{"tstamp": 1717712762.0426, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712761.4316, "finish": 1717712762.0426, "state": {"conv_id": "2c7e6cf8b25f4f849d9b0f8e54a5af95", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack."], "output": null}, "ip": null}
{"tstamp": 1717712800.2933, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717712799.6578, "finish": 1717712800.2933, "state": {"conv_id": "d9c980523b334677b31aeed18a1649ed", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "Generative representational instruction tuning\nAll text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm."], "output": null}, "ip": null}
{"tstamp": 1717712800.2933, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717712799.6578, "finish": 1717712800.2933, "state": {"conv_id": "2c7e6cf8b25f4f849d9b0f8e54a5af95", "model_name": "intfloat/multilingual-e5-small", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "Generative representational instruction tuning\nAll text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm."], "output": null}, "ip": null}
{"tstamp": 1717713102.3454, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717713101.5338, "finish": 1717713102.3454, "state": {"conv_id": "d9c980523b334677b31aeed18a1649ed", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "Generative representational instruction tuning\nAll text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.", "The Hallmarks of Aging\nAging is characterized by a progressive loss of physiological integrity, leading to impaired function and increased vulnerability to death. This deterioration is the primary risk factor for major human pathologies, including cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Aging research has experienced an unprecedented advance over recent years, particularly with the discovery that the rate of aging is controlled, at least to some extent, by genetic pathways and biochemical processes conserved in evolution. This Review enumerates nine tentative hallmarks that represent common denominators of aging in different organisms, with special emphasis on mammalian aging. These hallmarks are: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication. A major challenge is to dissect the interconnectedness between the candidate hallmarks and their relative contributions to aging, with the final goal of identifying pharmaceutical targets to improve human health during aging, with minimal side effects."], "output": null}, "ip": null}
{"tstamp": 1717713102.3454, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717713101.5338, "finish": 1717713102.3454, "state": {"conv_id": "2c7e6cf8b25f4f849d9b0f8e54a5af95", "model_name": "intfloat/multilingual-e5-small", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "Generative representational instruction tuning\nAll text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.", "The Hallmarks of Aging\nAging is characterized by a progressive loss of physiological integrity, leading to impaired function and increased vulnerability to death. This deterioration is the primary risk factor for major human pathologies, including cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Aging research has experienced an unprecedented advance over recent years, particularly with the discovery that the rate of aging is controlled, at least to some extent, by genetic pathways and biochemical processes conserved in evolution. This Review enumerates nine tentative hallmarks that represent common denominators of aging in different organisms, with special emphasis on mammalian aging. These hallmarks are: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication. A major challenge is to dissect the interconnectedness between the candidate hallmarks and their relative contributions to aging, with the final goal of identifying pharmaceutical targets to improve human health during aging, with minimal side effects."], "output": null}, "ip": null}
{"tstamp": 1717713145.7666, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717713144.7538, "finish": 1717713145.7666, "state": {"conv_id": "d9c980523b334677b31aeed18a1649ed", "model_name": "intfloat/multilingual-e5-small", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "Generative representational instruction tuning\nAll text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.", "The Hallmarks of Aging\nAging is characterized by a progressive loss of physiological integrity, leading to impaired function and increased vulnerability to death. This deterioration is the primary risk factor for major human pathologies, including cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Aging research has experienced an unprecedented advance over recent years, particularly with the discovery that the rate of aging is controlled, at least to some extent, by genetic pathways and biochemical processes conserved in evolution. This Review enumerates nine tentative hallmarks that represent common denominators of aging in different organisms, with special emphasis on mammalian aging. These hallmarks are: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication. A major challenge is to dissect the interconnectedness between the candidate hallmarks and their relative contributions to aging, with the final goal of identifying pharmaceutical targets to improve human health during aging, with minimal side effects.", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\nRecent epidemiological studies in Sweden, a country with traditionally high milk consumption, revealed that the intake of non-fermented pasteurized milk increased all-cause mortality in a dose-dependent manner. In contrast, the majority of epidemiological and clinical studies report beneficial health effects of fermented milk products, especially of yogurt. It is the intention of this review to delineate potential molecular aging mechanisms related to the intake of non-fermented milk versus yogurt on the basis of mechanistic target of rapamycin complex 1 (mTORC1) signaling. Non-fermented pasteurized milk via its high bioavailability of insulinotropic branched-chain amino acids (BCAAs), abundance of lactose (glucosyl-galactose) and bioactive exosomal microRNAs (miRs) enhances mTORC1 signaling, which shortens lifespan and increases all-cause mortality. In contrast, fermentation-associated lactic acid bacteria metabolize BCAAs and degrade galactose and milk exosomes including their mTORC1-activating microRNAs. The Industrial Revolution, with the introduction of pasteurization and refrigeration of milk, restricted the action of beneficial milk-fermenting bacteria, which degrade milk\u2019s BCAAs, galactose and bioactive miRs that synergistically activate mTORC1. This unrecognized behavior change in humans after the Neolithic revolution increased aging-related over-activation of mTORC1 signaling in humans, who persistently consume large quantities of non-fermented pasteurized cow\u2019s milk, a potential risk factor for aging and all-cause mortality."], "output": null}, "ip": null}
{"tstamp": 1717713145.7666, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717713144.7538, "finish": 1717713145.7666, "state": {"conv_id": "2c7e6cf8b25f4f849d9b0f8e54a5af95", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "Generative representational instruction tuning\nAll text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.", "The Hallmarks of Aging\nAging is characterized by a progressive loss of physiological integrity, leading to impaired function and increased vulnerability to death. This deterioration is the primary risk factor for major human pathologies, including cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Aging research has experienced an unprecedented advance over recent years, particularly with the discovery that the rate of aging is controlled, at least to some extent, by genetic pathways and biochemical processes conserved in evolution. This Review enumerates nine tentative hallmarks that represent common denominators of aging in different organisms, with special emphasis on mammalian aging. These hallmarks are: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication. A major challenge is to dissect the interconnectedness between the candidate hallmarks and their relative contributions to aging, with the final goal of identifying pharmaceutical targets to improve human health during aging, with minimal side effects.", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\nRecent epidemiological studies in Sweden, a country with traditionally high milk consumption, revealed that the intake of non-fermented pasteurized milk increased all-cause mortality in a dose-dependent manner. In contrast, the majority of epidemiological and clinical studies report beneficial health effects of fermented milk products, especially of yogurt. It is the intention of this review to delineate potential molecular aging mechanisms related to the intake of non-fermented milk versus yogurt on the basis of mechanistic target of rapamycin complex 1 (mTORC1) signaling. Non-fermented pasteurized milk via its high bioavailability of insulinotropic branched-chain amino acids (BCAAs), abundance of lactose (glucosyl-galactose) and bioactive exosomal microRNAs (miRs) enhances mTORC1 signaling, which shortens lifespan and increases all-cause mortality. In contrast, fermentation-associated lactic acid bacteria metabolize BCAAs and degrade galactose and milk exosomes including their mTORC1-activating microRNAs. The Industrial Revolution, with the introduction of pasteurization and refrigeration of milk, restricted the action of beneficial milk-fermenting bacteria, which degrade milk\u2019s BCAAs, galactose and bioactive miRs that synergistically activate mTORC1. This unrecognized behavior change in humans after the Neolithic revolution increased aging-related over-activation of mTORC1 signaling in humans, who persistently consume large quantities of non-fermented pasteurized cow\u2019s milk, a potential risk factor for aging and all-cause mortality."], "output": null}, "ip": null}
{"tstamp": 1717713393.8248, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717713392.6998, "finish": 1717713393.8248, "state": {"conv_id": "d9c980523b334677b31aeed18a1649ed", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "Generative representational instruction tuning\nAll text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.", "The Hallmarks of Aging\nAging is characterized by a progressive loss of physiological integrity, leading to impaired function and increased vulnerability to death. This deterioration is the primary risk factor for major human pathologies, including cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Aging research has experienced an unprecedented advance over recent years, particularly with the discovery that the rate of aging is controlled, at least to some extent, by genetic pathways and biochemical processes conserved in evolution. This Review enumerates nine tentative hallmarks that represent common denominators of aging in different organisms, with special emphasis on mammalian aging. These hallmarks are: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication. A major challenge is to dissect the interconnectedness between the candidate hallmarks and their relative contributions to aging, with the final goal of identifying pharmaceutical targets to improve human health during aging, with minimal side effects.", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\nRecent epidemiological studies in Sweden, a country with traditionally high milk consumption, revealed that the intake of non-fermented pasteurized milk increased all-cause mortality in a dose-dependent manner. In contrast, the majority of epidemiological and clinical studies report beneficial health effects of fermented milk products, especially of yogurt. It is the intention of this review to delineate potential molecular aging mechanisms related to the intake of non-fermented milk versus yogurt on the basis of mechanistic target of rapamycin complex 1 (mTORC1) signaling. Non-fermented pasteurized milk via its high bioavailability of insulinotropic branched-chain amino acids (BCAAs), abundance of lactose (glucosyl-galactose) and bioactive exosomal microRNAs (miRs) enhances mTORC1 signaling, which shortens lifespan and increases all-cause mortality. In contrast, fermentation-associated lactic acid bacteria metabolize BCAAs and degrade galactose and milk exosomes including their mTORC1-activating microRNAs. The Industrial Revolution, with the introduction of pasteurization and refrigeration of milk, restricted the action of beneficial milk-fermenting bacteria, which degrade milk\u2019s BCAAs, galactose and bioactive miRs that synergistically activate mTORC1. This unrecognized behavior change in humans after the Neolithic revolution increased aging-related over-activation of mTORC1 signaling in humans, who persistently consume large quantities of non-fermented pasteurized cow\u2019s milk, a potential risk factor for aging and all-cause mortality.", "Calorie Restriction Promotes Mammalian Cell Survival by Inducing the SIRT1 Deacetylase\nA major cause of aging is thought to result from the cumulative effects of cell loss over time. In yeast, caloric restriction (CR) delays aging by activating the Sir2 deacetylase. Here we show that expression of mammalian Sir2 (SIRT1) is induced in CR rats as well as in human cells that are treated with serum from these animals. Insulin and insulin-like growth factor 1 (IGF-1) attenuated this response. SIRT1 deacetylates the DNA repair factor Ku70, causing it to sequester the proapoptotic factor Bax away from mitochondria, thereby inhibiting stress-induced apoptotic cell death. Thus, CR could extend life-span by inducing SIRT1 expression and promoting the long-term survival of irreplaceable cells."], "output": null}, "ip": null}
{"tstamp": 1717713393.8248, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717713392.6998, "finish": 1717713393.8248, "state": {"conv_id": "2c7e6cf8b25f4f849d9b0f8e54a5af95", "model_name": "intfloat/multilingual-e5-small", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "Generative representational instruction tuning\nAll text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.", "The Hallmarks of Aging\nAging is characterized by a progressive loss of physiological integrity, leading to impaired function and increased vulnerability to death. This deterioration is the primary risk factor for major human pathologies, including cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Aging research has experienced an unprecedented advance over recent years, particularly with the discovery that the rate of aging is controlled, at least to some extent, by genetic pathways and biochemical processes conserved in evolution. This Review enumerates nine tentative hallmarks that represent common denominators of aging in different organisms, with special emphasis on mammalian aging. These hallmarks are: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication. A major challenge is to dissect the interconnectedness between the candidate hallmarks and their relative contributions to aging, with the final goal of identifying pharmaceutical targets to improve human health during aging, with minimal side effects.", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\nRecent epidemiological studies in Sweden, a country with traditionally high milk consumption, revealed that the intake of non-fermented pasteurized milk increased all-cause mortality in a dose-dependent manner. In contrast, the majority of epidemiological and clinical studies report beneficial health effects of fermented milk products, especially of yogurt. It is the intention of this review to delineate potential molecular aging mechanisms related to the intake of non-fermented milk versus yogurt on the basis of mechanistic target of rapamycin complex 1 (mTORC1) signaling. Non-fermented pasteurized milk via its high bioavailability of insulinotropic branched-chain amino acids (BCAAs), abundance of lactose (glucosyl-galactose) and bioactive exosomal microRNAs (miRs) enhances mTORC1 signaling, which shortens lifespan and increases all-cause mortality. In contrast, fermentation-associated lactic acid bacteria metabolize BCAAs and degrade galactose and milk exosomes including their mTORC1-activating microRNAs. The Industrial Revolution, with the introduction of pasteurization and refrigeration of milk, restricted the action of beneficial milk-fermenting bacteria, which degrade milk\u2019s BCAAs, galactose and bioactive miRs that synergistically activate mTORC1. This unrecognized behavior change in humans after the Neolithic revolution increased aging-related over-activation of mTORC1 signaling in humans, who persistently consume large quantities of non-fermented pasteurized cow\u2019s milk, a potential risk factor for aging and all-cause mortality.", "Calorie Restriction Promotes Mammalian Cell Survival by Inducing the SIRT1 Deacetylase\nA major cause of aging is thought to result from the cumulative effects of cell loss over time. In yeast, caloric restriction (CR) delays aging by activating the Sir2 deacetylase. Here we show that expression of mammalian Sir2 (SIRT1) is induced in CR rats as well as in human cells that are treated with serum from these animals. Insulin and insulin-like growth factor 1 (IGF-1) attenuated this response. SIRT1 deacetylates the DNA repair factor Ku70, causing it to sequester the proapoptotic factor Bax away from mitochondria, thereby inhibiting stress-induced apoptotic cell death. Thus, CR could extend life-span by inducing SIRT1 expression and promoting the long-term survival of irreplaceable cells."], "output": null}, "ip": null}
{"tstamp": 1717713432.8441, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717713431.7266, "finish": 1717713432.8441, "state": {"conv_id": "d9c980523b334677b31aeed18a1649ed", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "Generative representational instruction tuning\nAll text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.", "The Hallmarks of Aging\nAging is characterized by a progressive loss of physiological integrity, leading to impaired function and increased vulnerability to death. This deterioration is the primary risk factor for major human pathologies, including cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Aging research has experienced an unprecedented advance over recent years, particularly with the discovery that the rate of aging is controlled, at least to some extent, by genetic pathways and biochemical processes conserved in evolution. This Review enumerates nine tentative hallmarks that represent common denominators of aging in different organisms, with special emphasis on mammalian aging. These hallmarks are: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication. A major challenge is to dissect the interconnectedness between the candidate hallmarks and their relative contributions to aging, with the final goal of identifying pharmaceutical targets to improve human health during aging, with minimal side effects.", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\nRecent epidemiological studies in Sweden, a country with traditionally high milk consumption, revealed that the intake of non-fermented pasteurized milk increased all-cause mortality in a dose-dependent manner. In contrast, the majority of epidemiological and clinical studies report beneficial health effects of fermented milk products, especially of yogurt. It is the intention of this review to delineate potential molecular aging mechanisms related to the intake of non-fermented milk versus yogurt on the basis of mechanistic target of rapamycin complex 1 (mTORC1) signaling. Non-fermented pasteurized milk via its high bioavailability of insulinotropic branched-chain amino acids (BCAAs), abundance of lactose (glucosyl-galactose) and bioactive exosomal microRNAs (miRs) enhances mTORC1 signaling, which shortens lifespan and increases all-cause mortality. In contrast, fermentation-associated lactic acid bacteria metabolize BCAAs and degrade galactose and milk exosomes including their mTORC1-activating microRNAs. The Industrial Revolution, with the introduction of pasteurization and refrigeration of milk, restricted the action of beneficial milk-fermenting bacteria, which degrade milk\u2019s BCAAs, galactose and bioactive miRs that synergistically activate mTORC1. This unrecognized behavior change in humans after the Neolithic revolution increased aging-related over-activation of mTORC1 signaling in humans, who persistently consume large quantities of non-fermented pasteurized cow\u2019s milk, a potential risk factor for aging and all-cause mortality.", "Calorie Restriction Promotes Mammalian Cell Survival by Inducing the SIRT1 Deacetylase\nA major cause of aging is thought to result from the cumulative effects of cell loss over time. In yeast, caloric restriction (CR) delays aging by activating the Sir2 deacetylase. Here we show that expression of mammalian Sir2 (SIRT1) is induced in CR rats as well as in human cells that are treated with serum from these animals. Insulin and insulin-like growth factor 1 (IGF-1) attenuated this response. SIRT1 deacetylates the DNA repair factor Ku70, causing it to sequester the proapoptotic factor Bax away from mitochondria, thereby inhibiting stress-induced apoptotic cell death. Thus, CR could extend life-span by inducing SIRT1 expression and promoting the long-term survival of irreplaceable cells.", "Extending Healthy Life Span\u2014From Yeast to Humans\nWhen the food intake of organisms such as yeast and rodents is reduced (dietary restriction), they live longer than organisms fed a normal diet. A similar effect is seen when the activity of nutrient-sensing pathways is reduced by mutations or chemical inhibitors. In rodents, both dietary restriction and decreased nutrient-sensing pathway activity can lower the incidence of age-related loss of function and disease, including tumors and neurodegeneration. Dietary restriction also increases life span and protects against diabetes, cancer, and cardiovascular disease in rhesus monkeys, and in humans it causes changes that protect against these age-related pathologies. Tumors and diabetes are also uncommon in humans with mutations in the growth hormone receptor, and natural genetic variants in nutrient-sensing pathways are associated with increased human life span. Dietary restriction and reduced activity of nutrient-sensing pathways may thus slow aging by similar mechanisms, which have been conserved during evolution. We discuss these findings and their potential application to prevention of age-related disease and promotion of healthy aging in humans, and the challenge of possible negative side effects.\n"], "output": null}, "ip": null}
{"tstamp": 1717713432.8441, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717713431.7266, "finish": 1717713432.8441, "state": {"conv_id": "2c7e6cf8b25f4f849d9b0f8e54a5af95", "model_name": "intfloat/multilingual-e5-small", "prompt": ["SGPT: GPT sentence embeddings for semantic search\nDecoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.", "Crosslingual generalization through multitask finetuning\nMultitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.", "MTEB: Massive text embedding benchmark\nText embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.", "Scaling Data-Constrained Language Models\nThe current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github. com/huggingface/datablations.", "Octopack: Instruction tuning code large language models\nFinetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "Generative representational instruction tuning\nAll text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.", "The Hallmarks of Aging\nAging is characterized by a progressive loss of physiological integrity, leading to impaired function and increased vulnerability to death. This deterioration is the primary risk factor for major human pathologies, including cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Aging research has experienced an unprecedented advance over recent years, particularly with the discovery that the rate of aging is controlled, at least to some extent, by genetic pathways and biochemical processes conserved in evolution. This Review enumerates nine tentative hallmarks that represent common denominators of aging in different organisms, with special emphasis on mammalian aging. These hallmarks are: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication. A major challenge is to dissect the interconnectedness between the candidate hallmarks and their relative contributions to aging, with the final goal of identifying pharmaceutical targets to improve human health during aging, with minimal side effects.", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\nRecent epidemiological studies in Sweden, a country with traditionally high milk consumption, revealed that the intake of non-fermented pasteurized milk increased all-cause mortality in a dose-dependent manner. In contrast, the majority of epidemiological and clinical studies report beneficial health effects of fermented milk products, especially of yogurt. It is the intention of this review to delineate potential molecular aging mechanisms related to the intake of non-fermented milk versus yogurt on the basis of mechanistic target of rapamycin complex 1 (mTORC1) signaling. Non-fermented pasteurized milk via its high bioavailability of insulinotropic branched-chain amino acids (BCAAs), abundance of lactose (glucosyl-galactose) and bioactive exosomal microRNAs (miRs) enhances mTORC1 signaling, which shortens lifespan and increases all-cause mortality. In contrast, fermentation-associated lactic acid bacteria metabolize BCAAs and degrade galactose and milk exosomes including their mTORC1-activating microRNAs. The Industrial Revolution, with the introduction of pasteurization and refrigeration of milk, restricted the action of beneficial milk-fermenting bacteria, which degrade milk\u2019s BCAAs, galactose and bioactive miRs that synergistically activate mTORC1. This unrecognized behavior change in humans after the Neolithic revolution increased aging-related over-activation of mTORC1 signaling in humans, who persistently consume large quantities of non-fermented pasteurized cow\u2019s milk, a potential risk factor for aging and all-cause mortality.", "Calorie Restriction Promotes Mammalian Cell Survival by Inducing the SIRT1 Deacetylase\nA major cause of aging is thought to result from the cumulative effects of cell loss over time. In yeast, caloric restriction (CR) delays aging by activating the Sir2 deacetylase. Here we show that expression of mammalian Sir2 (SIRT1) is induced in CR rats as well as in human cells that are treated with serum from these animals. Insulin and insulin-like growth factor 1 (IGF-1) attenuated this response. SIRT1 deacetylates the DNA repair factor Ku70, causing it to sequester the proapoptotic factor Bax away from mitochondria, thereby inhibiting stress-induced apoptotic cell death. Thus, CR could extend life-span by inducing SIRT1 expression and promoting the long-term survival of irreplaceable cells.", "Extending Healthy Life Span\u2014From Yeast to Humans\nWhen the food intake of organisms such as yeast and rodents is reduced (dietary restriction), they live longer than organisms fed a normal diet. A similar effect is seen when the activity of nutrient-sensing pathways is reduced by mutations or chemical inhibitors. In rodents, both dietary restriction and decreased nutrient-sensing pathway activity can lower the incidence of age-related loss of function and disease, including tumors and neurodegeneration. Dietary restriction also increases life span and protects against diabetes, cancer, and cardiovascular disease in rhesus monkeys, and in humans it causes changes that protect against these age-related pathologies. Tumors and diabetes are also uncommon in humans with mutations in the growth hormone receptor, and natural genetic variants in nutrient-sensing pathways are associated with increased human life span. Dietary restriction and reduced activity of nutrient-sensing pathways may thus slow aging by similar mechanisms, which have been conserved during evolution. We discuss these findings and their potential application to prevention of age-related disease and promotion of healthy aging in humans, and the challenge of possible negative side effects.\n"], "output": null}, "ip": null}
{"tstamp": 1717713962.4475, "task_type": "retrieval", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717713961.9151, "finish": 1717713962.4475, "state": {"conv_id": "fe85f4ee098e4277850f93a9e05ebefa", "model_name": "intfloat/multilingual-e5-small", "prompt": "Crosslingual generalization through multitask finetuning", "output": [["Crosslingual generalization through multitask finetuning", "Title: Multilayer perceptron\n\nPassage: Using gradient descent, the change in each weight is"]]}, "ip": null}
{"tstamp": 1717713962.4475, "task_type": "retrieval", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717713961.9151, "finish": 1717713962.4475, "state": {"conv_id": "5eab1050c14143c799bda512380c5766", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Crosslingual generalization through multitask finetuning", "output": [["Crosslingual generalization through multitask finetuning", "Title: Tobacco advertising\n\nPassage: Both Google and Microsoft have policies that prohibit the promotion of tobacco products on their advertising networks.[23][24] However, some tobacco retailers are able to circumvent these policies by creating landing pages that promote tobacco accessories such as cigar humidors and lighters."]]}, "ip": null}
{"tstamp": 1717713968.3123, "task_type": "retrieval", "type": "leftvote", "models": ["", ""], "states": [{"conv_id": "fe85f4ee098e4277850f93a9e05ebefa", "model_name": "intfloat/multilingual-e5-small", "prompt": "Crosslingual generalization through multitask finetuning", "output": [["Crosslingual generalization through multitask finetuning", "Title: Multilayer perceptron\n\nPassage: Using gradient descent, the change in each weight is"]]}, {"conv_id": "5eab1050c14143c799bda512380c5766", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": "Crosslingual generalization through multitask finetuning", "output": [["Crosslingual generalization through multitask finetuning", "Title: Tobacco advertising\n\nPassage: Both Google and Microsoft have policies that prohibit the promotion of tobacco products on their advertising networks.[23][24] However, some tobacco retailers are able to circumvent these policies by creating landing pages that promote tobacco accessories such as cigar humidors and lighters."]]}], "ip": null}
{"tstamp": 1717713973.006, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717713972.5485, "finish": 1717713973.006, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning"], "output": null}, "ip": null}
{"tstamp": 1717713973.006, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717713972.5485, "finish": 1717713973.006, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning"], "output": null}, "ip": null}
{"tstamp": 1717713980.0497, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717713979.5018, "finish": 1717713980.0497, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark"], "output": null}, "ip": null}
{"tstamp": 1717713980.0497, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717713979.5018, "finish": 1717713980.0497, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark"], "output": null}, "ip": null}
{"tstamp": 1717713987.4446, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717713987.0494, "finish": 1717713987.4446, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search"], "output": null}, "ip": null}
{"tstamp": 1717713987.4446, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717713987.0494, "finish": 1717713987.4446, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search"], "output": null}, "ip": null}
{"tstamp": 1717713993.7044, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717713993.4888, "finish": 1717713993.7044, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models"], "output": null}, "ip": null}
{"tstamp": 1717713993.7044, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717713993.4888, "finish": 1717713993.7044, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models"], "output": null}, "ip": null}
{"tstamp": 1717713999.507, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717713999.3305, "finish": 1717713999.507, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models"], "output": null}, "ip": null}
{"tstamp": 1717713999.507, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717713999.3305, "finish": 1717713999.507, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models"], "output": null}, "ip": null}
{"tstamp": 1717714010.9554, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717714010.5522, "finish": 1717714010.9554, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning"], "output": null}, "ip": null}
{"tstamp": 1717714010.9554, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717714010.5522, "finish": 1717714010.9554, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning"], "output": null}, "ip": null}
{"tstamp": 1717714038.9579, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717714038.11, "finish": 1717714038.9579, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans"], "output": null}, "ip": null}
{"tstamp": 1717714038.9579, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717714038.11, "finish": 1717714038.9579, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans"], "output": null}, "ip": null}
{"tstamp": 1717714044.6286, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717714044.1847, "finish": 1717714044.6286, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\n"], "output": null}, "ip": null}
{"tstamp": 1717714044.6286, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717714044.1847, "finish": 1717714044.6286, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\n"], "output": null}, "ip": null}
{"tstamp": 1717714061.5432, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717714061.0352, "finish": 1717714061.5432, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\n", "The Hallmarks of Aging"], "output": null}, "ip": null}
{"tstamp": 1717714061.5432, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717714061.0352, "finish": 1717714061.5432, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\n", "The Hallmarks of Aging"], "output": null}, "ip": null}
{"tstamp": 1717714073.4752, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717714072.9492, "finish": 1717714073.4752, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\n", "The Hallmarks of Aging", "Resveratrol improves health and survival of mice on a high-calorie diet"], "output": null}, "ip": null}
{"tstamp": 1717714073.4752, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717714072.9492, "finish": 1717714073.4752, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\n", "The Hallmarks of Aging", "Resveratrol improves health and survival of mice on a high-calorie diet"], "output": null}, "ip": null}
{"tstamp": 1717714203.4226, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717714202.7025, "finish": 1717714203.4226, "state": {"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\n", "The Hallmarks of Aging", "Resveratrol improves health and survival of mice on a high-calorie diet", "Induction of pluripotent stem cells from mouse embryonic and adult fibroblast cultures by defined factors"], "output": null}, "ip": null}
{"tstamp": 1717714203.4226, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717714202.7025, "finish": 1717714203.4226, "state": {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\n", "The Hallmarks of Aging", "Resveratrol improves health and survival of mice on a high-calorie diet", "Induction of pluripotent stem cells from mouse embryonic and adult fibroblast cultures by defined factors"], "output": null}, "ip": null}
{"tstamp": 1717714628.5518, "task_type": "clustering", "type": "rightvote", "models": ["", ""], "states": [{"conv_id": "2e11f5b3a50b4bab877d27e25cb8570d", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\n", "The Hallmarks of Aging", "Resveratrol improves health and survival of mice on a high-calorie diet", "Induction of pluripotent stem cells from mouse embryonic and adult fibroblast cultures by defined factors"], "output": null}, {"conv_id": "07da27989ad94d0abb0ebc2261f97f63", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning", "MTEB: Massive text embedding benchmark", "SGPT: GPT sentence embeddings for semantic search", "Scaling Data-Constrained Language Models", "Octopack: Instruction tuning code large language models", "Generative representational instruction tuning", "Extending Healthy Life Span\u2014From Yeast to Humans", "Pasteurized non-fermented cow\u2019s milk but not fermented milk is a promoter of mTORC1-driven aging and increased mortality\n", "The Hallmarks of Aging", "Resveratrol improves health and survival of mice on a high-calorie diet", "Induction of pluripotent stem cells from mouse embryonic and adult fibroblast cultures by defined factors"], "output": null}], "ip": null}
{"tstamp": 1717715207.0865, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717715202.9291, "finish": 1717715207.0865, "state": {"conv_id": "3fb89f51c8974329a09c1fb18dbe07bb", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Bloom: A 176b-parameter open-access multilingual language model"], "output": null}, "ip": null}
{"tstamp": 1717715207.0865, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717715202.9291, "finish": 1717715207.0865, "state": {"conv_id": "fe6bcc5e0ba74eca8d33952d1ef57062", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Bloom: A 176b-parameter open-access multilingual language model"], "output": null}, "ip": null}
{"tstamp": 1717715210.7417, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717715210.52, "finish": 1717715210.7417, "state": {"conv_id": "3fb89f51c8974329a09c1fb18dbe07bb", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Bloom: A 176b-parameter open-access multilingual language model", "StarCoder: may the source be with you!"], "output": null}, "ip": null}
{"tstamp": 1717715210.7417, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717715210.52, "finish": 1717715210.7417, "state": {"conv_id": "fe6bcc5e0ba74eca8d33952d1ef57062", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Bloom: A 176b-parameter open-access multilingual language model", "StarCoder: may the source be with you!"], "output": null}, "ip": null}
{"tstamp": 1717715218.6248, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717715218.4215, "finish": 1717715218.6248, "state": {"conv_id": "3fb89f51c8974329a09c1fb18dbe07bb", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Bloom: A 176b-parameter open-access multilingual language model", "StarCoder: may the source be with you!", "Crosslingual generalization through multitask finetuning"], "output": null}, "ip": null}
{"tstamp": 1717715218.6248, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717715218.4215, "finish": 1717715218.6248, "state": {"conv_id": "fe6bcc5e0ba74eca8d33952d1ef57062", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Bloom: A 176b-parameter open-access multilingual language model", "StarCoder: may the source be with you!", "Crosslingual generalization through multitask finetuning"], "output": null}, "ip": null}
{"tstamp": 1717715244.4577, "task_type": "clustering", "type": "chat", "model": "intfloat/multilingual-e5-small", "gen_params": {}, "start": 1717715244.0037, "finish": 1717715244.4577, "state": {"conv_id": "2eaf194a52564a68bffb4d567703bf3a", "model_name": "intfloat/multilingual-e5-small", "prompt": ["Crosslingual generalization through multitask finetuning"], "output": null}, "ip": null}
{"tstamp": 1717715244.4577, "task_type": "clustering", "type": "chat", "model": "sentence-transformers/all-MiniLM-L6-v2", "gen_params": {}, "start": 1717715244.0037, "finish": 1717715244.4577, "state": {"conv_id": "821df2a4609642ba9919f1a4fa1538de", "model_name": "sentence-transformers/all-MiniLM-L6-v2", "prompt": ["Crosslingual generalization through multitask finetuning"], "output": null}, "ip": null}
